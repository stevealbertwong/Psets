70: Stanford, MIT, Cal
63: Caltec, Carneige Mellon, Princeton, Harvard
56: Columbia, Cornell, UPenn, Maryland, Michigan, Georgia Tech, Waterloo
51: UChicago, UT, Washington, UIUC, UCLA, RIT, Harvey Mudd, Middlebury
47: Northwestern, Duke, JohnHopkins, UVA, Vanderbilt, USC, Lehigh, UCSD, Florida
42: UMassAmherst, UNC, Buffalo, Davis, Arizona, FSU, Purdue, Stony Brook, VT, Rugters
35: Temple, Penn State, Drexel, Worcester Polytech, TCNJ, NJIT, SMU, UT Dallas, SUNY Binghamton  

>10: A+
9: A
8: A-
7: B+
6: B
5: B-

INTRO
cs103 4 weeks
cs41 2 weeks
cs106a 2 weeks
cs106b 3 weeks
cs106l 2 weeks
cs106x 2 weeks
cs108 3 weeks
cs109 4 weeks

SYSTEM:
cs1u 1 week
cs107 4 weeks 
cs110 4 weeks 
cs140 6 weeks 
cs240 4 weeks
cs143 4 weeks
cs144 4 weeks
cs244 4 weeks
cs155 4 weeks

DATA STRUCTURE
cs161 4 weeks
cs261 4 weeks
cs166 4 weeks
cs168 4 weeks
cs167 4 weeks
cs267 4 weeks

MACHINE LEARNING
cs228 4 weeks
cs221 4 weeks
Math51 4 weeks
cs229 4 weeks
cs231n 4 weeks
cs224d 3 weeks
cs224u 3 weeks
cs224n 3 weeks
Convex Optimization 5 weeks

MOBILE AND WEB
cs193p 4 weeks
cs193a 3 weeks
cs142 3 weeks

OTHERS
cs124 1 week
cs276 1 week
cs246 4 weeks
cs224w
21 months


Agile Development: 
-1 hour work = 1 pt

5-28: 
Read up assign7 -> 4 pts
Read up CMU malloclab ->2 pts

5-29:
mycodeschool pointer arithmetic, type casting, void pointer -> 2 pts
Learn C pointers and struct and data structure -> 2 pts
mycodeschool malloc, realloc, calloc, free() -> 3 pts

5-30:
finish up malloc recitiation ->3 pts
starting reading on 110 mapreduce -> 1pt

5-31:
reading 110 mapreduce-server.cc -> 3 pts
MapReduceServer Class 
unsigned long extractPortNumber(const char* portArgument)
void initializeFromConfigFile(const std::string& configFilename)
void applyToServer(const std::string& key, const std::string& value)
std::string ensureDirectoryExists(const std::string& key, const std::string& path)

6-1:
110 mapreduce-server.cc
buildIPAddressMap ->1pt

6-2:
106b LinkedList::add, get, insert - 3pts

6-3:
15-213 Virtual Memory6-3:

6-4: 
106b BinaryTree::print, contains - 2pts
110 int createServerSocket(unsigned short port, in backlog) - 1pt

6-5:
learn c ex18 function pointer as callback -1pt
typedef int (*compare_callback)(int a, int b);
int* bubble_sort(int a, int b, compare_callback c){
	if c(a, b)>0{
	do something..}
};
 
106b file/io ifstream getline(input, file), open(), close()- 1pt

6-6:
makefile, files settings dependencies, libraries -2pts
command make reads in the file Makefile from current directory
make reassemble, make myprogram, make clean -> build specific target 
gcc main.cc function1.cc function2.cc -o hello

109 - pset5 titanic + estimating parameter -3pts
import csv
open("titanic.csv") as csvfile
readCSV= csv.reader(csvfile, delimiter=",")
for row in readCSV // go through each row
	if (row[3] == "w" && row[1] == "1") // go through each item in the row to get woman in first class
		women1stClass++
		if  (row[0] == 1)
			survived ++
p = survived / women1stClass

6-7
109 parameter estimation, method of moments applied on poisson, bernoulli, normal, uniform distribution - 2pts
106b FirstProgram.cpp HamDaDaDa.cpp -3pts 
using namespace std = std::cout 
C++ - #include<iostream>, no public private static shit in c++
java - printl("Hello" + "world); #import, public static int main();
c++ - std::cout<< "hello" << "world" << std::endl // standard console output, pacman you are eating the output or less than less than, <<endl if not written then cursor just stay in the same line  
c - printf
getline();
int main() return 0 -> C++ the int goes out to operating system whether program has executed or not, in java main is void 
parametr sends information into method

wget download all index on apache website-> 1pt

6-8
Matrix review - reduce echelon form, LU factorization, linearly dependent, inverse, diagonal, upper triangular, gaussian elimination, homogeneous system with trivial solution, generic system, augmented matrix, rank of matrix, gauss-jordan elimination - 1pt

Google Developer tool https://www.youtube.com/watch?v=G74ll-6L4_o - 3pts
element, networking, resources, console
JS DataStructure and Algo - 1pt
143 coursera -1pt

6-9
VM setup but no login in page, repair file system error- 1pt
CSE 351 data lab - 1pt
351 lab0 -1pt
351 hardware organization, electronic implementation of bits, one memory address correspond to each byte of memory,8 bytes are going ot group tgt to represent 64 bit addressfor x86 machine, address is the first byte of grouped address, 64bits represents word size of the machine/integer size, so we could have 8 gbs different addresses,  
little vs big endian to order the byte within a word, IA32 vs x86
big -> most significant byte has lower address i.e. highest order digit 1 in 0x123456 has lowest address
little -> least significant byte has lower address x86-> little endian 
pointer is a data object that contains an address that you intepret as an address 
boolean in theory is 1 bit but in pratice we address data in 1 byte 

Array represents adjacent locations in memory storing same type of data object

6-10
CSE 351 - showbytes.c print address of pointer + hex representation of the value -1pt
351 - Boolean algebra + bit level manipulating + bitwise operations
DeMorgan's law ~(A|B) = ~A&~B intuition -> flip A/B flip to opposite i.e. ~A -> complement of A , | means 1+0 = 1, & means 1+0 =0 so we flip this rule as well
Bit-level operation
Logic operation -1pt
lab2 -1pt
x86 assembly -1pt


6-11
351 assembly movl %eax, (%edx) -2pts
351 buffer overflow lab3 -3 pts


6-12
351 stack and registers of void swap(int* x, int* y), moving data between registers and memory, 1st addressing mode: movl 12(%ebp), %eax i.e. traversing the offset on the memory side to get the value vs 2nd addressing mode: movl (%eax) %edx i.e. %eax storing the address so go from register side to address of memory side to get the value  - 2pts

Mining Massive Dataset -3pts
-distributed file system, cluster architecture: 1. chunk servers 2.master/name node 3.client library for file access 
-cluster computation challenge: node failure, network bandwidth, complexity -> solution: map-reduce
- e.g. hostname(URL), size -> find the size of paritcular hostname
- e.g. unique word, appearance times
- e.g. count number of times each 5-word sequence occur in a large corpus of documents
map: scan the document once, creates a map of key and value
group by key: sort in order
reduce: reduce/combine the duplicates/multiple occurances into unique occurance result so it can be run in a single node/rack
most examples querying a distributed file system to get a combined result

- link analysis algorithm to compute importances of a node in a graph
- e.g. page rank, hubs and authorities

Reading through quiz - 2pts
 
addressing mode e.g. leal, addl, imul, sall 4(%ecx, %eax) - 1 pt


6-13
351 gdb tutorial -3pt
break main, run, list, disas, step, stepi, help, help info, info registers, x /NUM SIZE FORMAT e.g. x /24wx $rsp i.e. 24 words in hex on the stack 
x /2wx $rdi, x /s $rdi x /x $rsi

phase 1 defused
working on phase 2
target record /target record-full + reverse-step/reverse-next BEFORE step AFTER run -> step back

161- merge sort, 3ways merge sort, x ways merge sort -1pt 
counting no. of inversions - brute force, steps it takes to become sorted array i.e. piggybanking, recursive brute force 

6-14
quick sort vs merge sort, randomized quick sort solving ith statistic element, randomly hoping pivot is going to end up as the ith order element we picked, only one side recursive call depending on which side the ith order is compared to the pivot we choose -3pts

graph vs sorting, social network, google map, course selection, meaningful connection between data not just ranking -2pts 
I still dont understand the number of cuts of n vertices equal to 2^n
right now I am thinking when a new node gets added, it will create an edge with all exist edges so each time the numbers of edges get double when a node is added.
adjacent list, max min amount of edges of n vertices = n-1, n(n-1)/2

DFS, getNeighbours, getVertex, neighbour->visited = true, neighbour->previous = vertC, 2014 mid term followers of followers -1pt


6-15
minimum cut problem - 2pts
if you figure this out, you could become a Stanford PHD.
161-PS3 -1 pt
BFS, DFS, Dijkstra's algo -4pts

DFS - Discovery guarantee to find a path if one exists + D if you apply it since only bactrace
- mark visited, for loop for neighbour node, if stuck backtrace.
Do Fuckup Shit

BFS - cooler than DFS as it will always find the shortest possible path + implementing queue is harder than backtrace, keep track of path by storing predecessor for each vertex, use up more memory assign2 word letter, breadcrump-> breadth = tried every shortest path
- queue, mark visited + set n's previous to be v (breadcrumb), for loop for neighbour node + enqueue, dequeue
Being Fucking Smart

Dijkstra's algo - Cooler than B cause you get a Turing Award, minimum weight path of graph -> create a table of information about currrently known best way to reach each vertex
mark visited + pqueue + weight

going through all the nodes, keep updating the shortest path to the nodes, if you are in the queue then you are waiting for updates on your minimum cost, if you are dequeued then it is confirmed that it is your minimum cost , if you are enqueue then it is the first time you are considered for minimum cost

priotiy queue can guarantee it is the shortest path when you dequeue for 2 reasons: 
1. the first node in the pqueue is guaranteed to be the shortest path to that node as all other indirect paths have at least higher cost 
2. it guarantees it updates the shortest path before dequeuing it as it consider all other shortest paths including its indirect path before considering dequeuing it

prime example of greedy algo


6-16
161 forum post - algo in real world - 1pt
forum post -PA1 -1 pt
228 PA1 - 3pts


6-17
EVERYTHING NORMALIZE BECOME PROBABILITY
WHAT and WHY PGM - conceptual intuition
FRONTEND of probability + independent of algo i.e. you wont learn any algo + modeling a world of uncertainties   

PGM - model, graphical models, random variables that represents probabilities, bayesian network-> directed graph


PGM is a framework -> very large amount of variables + significantly uncertain about the right answer to the result

Model-> declaration representation of understanding of the world, representation of computer that captures our understanding the world in terms of variables and their relationship

Think of the world as a large amount of random variables -> model relationship with joint distribution over possible assignment of certain random variables

Joint distribution: P(x_1, x_2, .... x_n) -> huge possible states, intrinsically exponentially large, ALSO think of it as a table e.g. intelligence, difficulty, grades -> probailitity of every combinations of these 3 random varaibles 

Independent parameter -> values are not determined by other parameters

Conditioning/observing assignment of Grade A -> eliminate all possible assignments of Grade B/C + normalizing/rebase aka reduced probability distribution

Marginalize -> getting rid of intelligence just sum up according to distribution over 

Factor -> a function or a table (x_1, x_2, ...x_n) take all possible assignments in cross-product space i.e. all possible combination of assignments -> gives a real value for each combination 
e.g. joint distribution is a factor a table fo random varaibles
e.g. CPD is a factor
Scope of the factors -> (x_1, x_2, ... x_n) i.e. arguments the factors takes

Factor product -> conditional probability
Factor maginalization -> same
Factor reduction -> same delete + renormalize


Bae's network -> random variables are represented by nodes

Mark's network -> 

Matlab PA1 FactorTutorial.m -2 pts
- implement factor data structure
- AssignmentToIndex([1,2,1],[2,2,2]) //index is 3, [1,1,1] index is 1, [2,1,2] index is 2 etc
- IndexToAssignment(7,[2,2,2]) //1,2,2
- GetValueofAssignment
- SetValueOfAssignment(misspellCar, [1 2 1],6)

PA2 - 2pts
PA3 -1pt


6-18

Structure CPD -1pt
Tabular representation - naive representation over parents I, D and child G, but real world many factors could cause grades or cough, no. of parents of CPD grows exponentially

Dynamic Bayesian Network -1pt
Distribution over trajectory 
Markov assumption - 
Semi markov model 
Template transition model 
Ground Bayesian Network/2 time slice Bayesian Network 

Contextual specific indepedence -

Conditional Independence in bayesian network + navie bayes -1pt



Markov random fields
- pairwise markov networks 
	>> affinity functions, combatabiility function, soft constraints
	>> local happiness to take a particular joint assignments




6-20
PGM PA3 -2pts

RunInference.m
Pairwise, triplet factors 
Similarity factor 


PA4 - 1pt
Belief Propagation message passing in cluster graph - 1pt
Clique Tree - Family Preservation + running intersection property -1pt
PA4 walkthrough by TA -1pt

6-21
PA4 -forum -2pts
Java import java.io.*; library-> package -> class -> methods -> properties -1pt
taxCalculator -1pt 
calendar class -1pt
106a array -1pt

6-22
1. create ec2 instance
choose EC2, create key pair, choose ubuntu as server for configuration, launch new t1-micro instance and get an instance number, go to EC2 dashboard instance get the hostname of remote machine you launched, amazon assigned a IP address and hostname to you and respond to your request with key pair

2. connect to instance    
connect with a standalone SSH client-> chmod 400 key-pair.pem-> access through public DNS/hostname
e.g. ssh -i key-pair.pem root@hostname-amazonaws.com


start up engineering hw2-1pt
done start up engineering -1pt
cache lab -1pt
15-213 triple for loop in c to represent matrix multiplication -1pt


6-22
java array-1pt
andriod activity cycle -2pts
purple rain -3pts
graphics + arraylist to creates random raindrop + recursion does not work since will only copy itself + GOval object will store the number of moves + raindrops = addRainDrops + tick = 0, tick++, if tick % 10 == 0 + RandomGenerator.getinstance().nextint(0,getWidth()); + GOval(x, y, DROP_SIZE, DROP_SIZE) + public GOval addraindrop(){return raindrop} raindrop.move(0,2)  

setup node in ams -1pt
cyberport server
andriod restful api -1pt

6-23
I
marty andriod -> api call and queries + xml to json, getting string from json, e.g. cat images link from api quries
showing the images on the screen -> 
GridLayout grid = findViewById(R.id.grid); 
grid.removeAllViews();
ImageView imgView = new ImageView(this);
ViewGroup.LayoutParams params = new ViewGroup.LayoutParams(ViewGroup.LayoutParams.WRAP_CONTENT)
imgView.setLayoutParams(params);
Picasso.with(this).load(url).resize(500,500).into(imgView); //fetch the pixals of the image to show on screen
grid.addView(imgView); 

click button ->
public void catClick(View view){
	Ion.with(this).load(url).asString().setCallback(new FutureCallBack<>(String)).onCompleted{processStringData()}} 


https://www.youtube.com/watch?v=WxhFq64FQzA&index=185&list=PLQipVWExdXm4fewEo9Qrbcvbdh3EXFSQg
set up nodejs server in cloud instance amazon ec2 

push the limit -andriod networking -1pt
	HttpURLConnection -> JSONObject

andriod setup -1pt
⌥F7 - find all places classes are used
⌃Space - complete
F1 - documentation



6-24

https://developers.google.com/identity/sign-in/android/backend-auth#send-the-id-token-to-your-server
authentication

- get backend server's OAuth 2.0 client ID + pass server's client ID to requestIdToken
	>> 
- after user signs in, retrieve user ID token 
	>> GoogleSignInResult result = Auth.GoogleSignInApi.getSignInApiFromIntent(data);
	>> GoogleSignInAccount acct = result.getSignInAccount();
	>> String idtoken = acct.getIdToken(); // needs to configure Google sign-in with requestIdToken();
	>> mIdTokenTextView.setText("this is your id token:" + idtoken);
	>> // create httpurlconnection to send idtoken to server
- send ID token to server with an HTTP post request
	HttpClient httpClient = new DefaultHttpClient();
	HttpPost httpPost = new HttpPost("backend.com/tokensignin");
	// encode httpPost url  
	List nameValuePairs = new ArrayList(1);
	nameValuePairs.add(new BasicNameValuePair("idToken:", idToken); // ??
	httpPost.setEntity(new UrlEncodedFormEntity(nameValuePairs); // ??
	HttpResponse response = httpClient.execute(httpPost);
	
	// get status code and response body
	int statusCode = response.getStatusLine().getStatusCode();
	String responseBody = EntityUtils.toString(response.getEntity()); 
	Log.i(TAG, "Signed in as:" + responseBody); // ?? TAG
- server verify ID token
	// using google API Client library
	// HTTP 200 response, its boday contains JSONformatted ID token claims
- server retrieve ID token from sub claim


marty database, sql, firebase -2pts
sql -> Select column From table Where population > 1000 
	INSERT INTO countries VALUES('SLD', 'ENG', 'T', 100);
Database managment system -> MYSQL vs PostgreSQL vs SQLite

http://web.stanford.edu/class/archive/cs/cs193a/cs193a.1122/05/
-> SQLite read
	SQLiteDatabase db = openOrCreateDatabase("name", MODE_PRIVATE, null);
	db.execSQL("INSERT INTO counties VALUES (xxx)"); // INSERT or DELETE
	Cursor cr = db.rawQuery("SELECT id, email FROM students", null); // Cursor -> iterate through result
	if (cr.moveToNext()){
		int id = cr.getInt(cr.getColumnIndex("id");	
		String email = cr.getString(cr.getColumnIndex("email"));
	while (cr.moveToNext());
		cr.close();

6-25
review express -2pts
- app.listen(80) vs http.createServer(app).listen(80)
	app.get("/login", middleware, function(req, res){res.send("Login page")}) 
	// when express server receives a get request and path "/login", if route is found, response object and 
	// request object are created to passed into parameters of callback function

express-route.js-1pt
response request object, jade embedded js template engine, res.sendfile, res.json, res.redirect -1pt


6-27
marty Firebase remote database - 4pts
alias ga="git add ."
sql to nosql table: use sql to make query vs use map.get 
mapception
add stanford android library
FIREBASE.setValue + security rules write: true WORKS 


build.gradle dependencies ??
Log.wtf ??

Java DataBase Connector library/connector/driver
- differnet connector depending on your particular kind of database e.g. mysql connector
- API for connecting to remote database in java code
- directly querying database in java from android device
- skip setting up server
- server is a web layer between client and db: client makes queries by contacting certain specific URLs, server code send appropriate data back 
- return a cursor object
	// load JDBC driver and connect to the server
	String url = "jbdc:mysql://server:port/databaseName";
	Connection connection = DriverManager.getConnection(url, username, password);
	// run the query 
	// resultset(java) is cursor(andriod)
	Statement statement = connection.createStatement();
	ResultSet resultset = statement.executeQuery("SELECT name FROM table WHERE name = martin"); // SQL
	while (resultset.next(){ resultset.getInt("column name")});


Firebase
CONNECTING TO THE DATA
- install firebase into android build.gradle dependencies + add file exclusion in build.gradle
- add internet permission to project
	// initialize Firebase in public void onCreate()
	Firebase.setAndroidContext(this);
	// after initialization -> create key value pairing
	firebase.child("name").setValue(value);
	
	// Firebase common methods
	fb.child("name") // return child data object
	fb.getKey("value") // return key of a given data value
	fb.getParent("key") // return map one level up
	fb.setValue(value, handler) // sets new data value
	fb.updateChildren(map, handler); // updates objects field i.e. children

6-28
Database principles e.g. craigslist -2pts
1. create a table when you want to query it -> SELECT query FROM table
2. ID is a way to export the row of information to other tables
3. sql for excel like data e.g. school personal data, stock data, bus schedule
4. json for categorical data, lightweight sent over internet e.g. craigslist, simple stock data   

auto-generated keys + firebase.child(simpsons/students).push() + @Override ??
setting, getting data, datasnap object -1pt 
implementing login, intent exchange between activities, startActivity(intent), query.addChildEventListener, Query query = gradesTable.orderByChild("student_email").equalTo(inputEmail) -1pt


6-29
TODO
- read ionic cookbook -> figure out database schema + intent exchange
- facebook database schema
- firebase documentation -> auth object


6-30

Berkeley Admission Quiz app -3pts
- convert int to string: String.viewof(int); // then pass to button.setText(string);
- convert string to int: Integer.paseInt(string); // from buttom.getText(); then compare number
- generate different random int + make sure int generated not the same

Wigets and layout and Text Fields and Container

Layout  
- layout:alignParentLeft / alignLeft=textView margin = 35dp / centerHorizontal  
- below=textView margin=140dp / alignTop=textView / alignBottom=textView (overlap the text view) / centerVertical

Event
- Properties -> onClick identifer -> puts the name of the method 

ImageView
ListView
GridView
RadioGroup
layout_alignParentX
layout_marginX
RelativeLayout

Activity - fundamental units of GUI, a single screen of UI
View - item that appears on screen in an activity e.g. wigets and layout
Wigets - button, text view
Event - click, timer, network data available

	
7-1
layout managers- rules that decide how to position each layout
Layout = ViewGroup = Layout manager = containers of view = Activity
- described in XML mirrored in Java code 
- nested layout/custom layout -> combinations of features
- inside each layout is wigets, attributes

Element(tag), attribute(additional info about the tag), value

<course name="CS193A" instructor="marty>

layout_margin
gridlayout - layout widget in lines of row and columns e.g. calculator app RARELY USED

relativelayout 
- each widget position relative to each other, you describe position of wiget relative to other wigets

linear layout
- layout wigets/views in a single line
- does not wrap around, falls off the screen if reach the edge of screen

xml is extensible html, you could make up any tags as you want <pizza>

gravity
- alignment direction widgets are pulled
- alignment in linear layout is called gravity
- any stuff inside me move to the other side -> button: layout_gravity="right"

weight
- size of the wigets, fraction of overall screen
- 1 out of 1 of all the space thats left, competing with the remaining space with widgets with layout_weight="1"

padding
- padding makes the widget bigger
- get the button a little bigger stretching it out
- after setting the layout width and height to "match_parent" or "wrap_content" or "100dp"
- dp: device pixel, dip: device independent pixel, sp: scaling pixel
- match_parent: same size as the layout containers the widget is inside

margin
- outside of the widget, extra space outside to separate it from others

tablelayout same as gridlayout

APPLICATION
Composite/nested layout
- LinearLayout 
layout_gravity: refers to the gravity of LinearLayout to its parent
gravity: refers to its sub-widgets' gravity
WHICH MEANS -> layout gravity:center = button layout_gravity:center


7-2
START COUNTING GETTING NEW NIKE BOOT
imageButton
- src="@drawable/img"
imageView

EditText + Toast.makeText(MainActivity.this, s, Toast.LENGTH_SHORT).show()
- editable text input box

Linsanity-1pt

Toggle between sunny pictures
RadioButton -> RadioGroup
if(view.getId() == R.id.sunny){imgView.setImageResource(R.drawable.sunny)

ListView
- ordered collection of selectable choices
- entries="@array/array" // set of options to appear on the list 

static list -1pt
- content is fixed and known before the app runs
- populate items in strings.xml -> then import items to activity_main.xml
- declare list elements in strings.xml res/values/strings.xml
- res/layout/activity_main.xml <ListView entries="@array/name">
- res/strings.xml <string-array name="name"> <item>MIT</item>

dynamic list -1pt
- content is read or generated as the program runs
- items of the list could come from file or java code
- list adapters

List adapters
- ArrayAdapter
- listView.setAdapter(adapter);
- listView.setOnItemClickListener(new AdapterView.OnItemClickListener(){public void onItemClick(AdapterView, View, position, id)


dictionary app -1pt
when make anonymous inner class new AdapterView, if you want inner object to see global variable -> add final in front of the global varaible, final = const
also Toast.makeText(this, "you suck", Toast.LENGTH_SHORT).show() this has to be changed to MainActivity.this -> usually writing code in activity class and this refers to the activity class but now this is inside a new object inside the activity class so the activity/object should be MainActivity.this to refer to MainActviity rather than the tiny little new object this

word-guessing game -1pt

Changes to List data


7-3

Reviewing Firebase + List view -1pt
Reading Firebase doc + sample app -1pt
https://github.com/firebase/friendlypix/tree/master/android/app/src/main/java/com/google/firebase/samples/apps/friendlypix

@Override
- if a class inherits a method from its superclass -> override that method provided its not final
e.g. add method to existing class for inheritance/extends




Firebase readup

https://www.firebase.com/docs/android/guide/user-auth.html#section-storing
- user is returned in callbacks on the client device
- user information returned contains a uid (a unique ID)
- value of auth varaible becomes defined -> no longer null i.e. auth.uid, auth.name

Authentication provider
- ref.addAuthStateListener, if(authData!=null) // user is logged in
- AuthData authData = ref.getAuth()


Email&Password Auth
https://www.firebase.com/docs/android/guide/login/password.html


Running firebase starter kit -1pt
https://codelabs.developers.google.com/codelabs/firebase-android/#3

Storing new user login into database
https://github.com/firebase/firebase-simple-login-java/tree/master/docs/v1
	myRef.child("users").child(user.getUid()).setValue(map);

Access to Firebase is configure by JSON in firebase console


Firebase UI to populate Firebase model
https://github.com/firebase/FirebaseUI-Android/blob/master/database/README.md
Recycler view
- Create a model class to represent database object
	// creating a model to push to Database
	Chat msg = new Chat("puf", "1234", "Hello FirebaseUI world!");
	ref.push().setValue(msg); // pushing an instance of Chat class

- custom recycler/list adapter to map a collection in database to Android
	// creating a list adapter to populate the RecycleView/ListView from Database
	mAdapter = new FirebaseRecyclerAdapter(
	
- create custom ViewHolder class
- 


7-4

Firebase Android Codelab -4pts
https://codelabs.developers.google.com/codelabs/firebase-android/#5
	mGoogleApiClient = new GoogleApiClient.Builder(this);
	Intent signInIntent = Auth.GoogleSignInApi.getSignInIntent(mGoogleApiClient);
	protected void onActivityResult(int requestCode, int resultCode, Intent data)
	GoogleSignInResult result = Auth.GoogleSignInApi.getSignInResultFromIntent(data);
	GoogleSignInAccount account = result.getSignInAccount();
- the problem of this database is there is only one model (Friendly Message) so each logged in user store their name and message into the only one same database -> 1 collection with everyone's message in it


Udacity Firebase datastructure-1pt
Udacity Android -0.5pt


7-5
Udacity 
Set up sunshine + 1.01 Add ListItem XML-1pt
Sunshine arrayadapter + listView.setAdapter -2pts 

Shop++ set up -2pt
1. Firebase console - create project, add firebase to android, create config file, apply google service plugin
2. add Firebase/Google library/SDK to build gradle (both app and project level)
3. set internet permission in Android Manifest


7-6
Shop++ listener + resquest response model -1pt
get the datasnapshot brings it back to textview -1pt
fragment_shopping_lists.xml includes single_active_list.xml <include layout="@layout/single_active_list" />
POJO -1pt
- write(create instance of shoppinglist class and pass in listname and owner value + setvalue(shoppinglist))- read 

git remote -v // show git repo
rm -fr .git // remove git init
git clone -b xxx http//xxx.git

reading up Firebase v2 startup fire-1pt




7-11
BaseActivity.java
- deals with login logout for every activities that requires you to login to view

Android Manifest.xml
<action android:name="android.intent.action.MAIN" /> -> this is the app's entry point
CATEGORY_LAUNCHER means it should appear in the Launcher as a top-level application, while CATEGORY_ALTERNATIVE means it should be included in a list of alternative actions the user can perform on a piece of data. -> entry point should be listed in the application launcher + Android does not grab whichever one appears first in the manifest but it starts with activity having CATEGORY_LAUNCHER + CATEGORY_LAUNCHER : The activity can be the initial activity of a task and is listed in the top-level application launcher.

LoginActivity -> MainActivity

2 ways to load screen from Activity.java:

1. Activity
setContentView(R.layout.activity_main);

2. Fragment
View rootView = inflater.inflate(R.layout.fragment_shopping_lists, container, false); // load GUI layout from XML
initializeScreen(rootView); // mListView = (ListView) rootView.findViewById(R.id.list_view_active_lists);

Move xml into a new fragment -> then linear layout contain the new fragment in the original xml 


7-12
ViewGroup vs Fragment

Fragment -> fully modularize activity

ArrayListAdapter
- public constructor: public ActiveListAdapter(Activity activity, Class<ShoppingList> modelClass, int modelLayout, Query ref, String encodedEmail)  
- populates textViewUsersShopping and textViewCreatedByUser

ShoppingListFragment -> ActiveListDetailsActivity



7-13
nike boot start afresh

POJO - model
- convert simple Java object into Firebase database JSON
- e.g. making the shopping list an object 

AddListItemDialogFragment
EditListDialogFragment

LayoutInflater -> run logic of dynamic UI instead of static UI in XML
Fragment -> getActivity() to get the Activity it is in 
getMenuInflater().inflate(R.menu.menu_list_details, menu);
View footer = getLayoutInflater().inflate(R.layout.footer_empty, null);
Layout inflater converts XML into java widget

View rootView = getAcitivity().getLayoutInflater().inflate(R.layout / mResource, null)




7-14
layoutinflater is just a converter of XML into View object -> NO event or listener or set/get method or lifecycle 


7-15
Firebase - ShoppingListFragment -> ActiveListDetailsActivity
updateChildren, setValue, push -1pt
AddListDialogFragment -1pt



7-16
ActiveListAdapter -1pt



7-17
Firebase data schema -1pt
denormalization -> x depth of JSON -> takes data seems to be related in different parts of the app if optimize display in these activities 
2 way relationship + many to many relationship -1pt


7-18
Data schema -2pts

7-19
CreateAccountActivity -1pt


7-20
Rasberry Pi + electronics + hardware - 1 whole day

7-21
LoginActivity -1pt
ActivityCycle -1pt
FragmentStatePagerAdapter-1pt

7-22
Bug: open a new old-platform database console + create a new gradle.properties -1pt

Bug: google-service.json -1pt
https://developers.google.com/mobile/add
delete old app -> select android + enter package name + get registered SHA-1


Bug: invalid firebase_url -1pt
gradle properties should be: UniqueFirebaseRootUrl = "https://stevenshoppinglist.firebaseio.com/"

Allows email and password authentication + drawing out forum logic -1pt

passing mEncodedEmail from MainActivity to AddlistDialogFragment by newInstance()-> fragment.setargument(bundle.putstring(encodedEmail) -1pt 




7-23
to add message to another fragment -1pt
NEED TO:
1. update shoppinglist model
2. update addshoppinglist() in addlistdialogfragment
3. shoppinglistfragment
4. activelistadapter
5. layout.xml
6. mainactivity and list item fragments

Populate view - 1pt
Trying to add a like button -1pt
Still trying to add a like button -1pt


7-24
like button -2pts
addShoppingList() in addListDialogFragment -1pt
Utils.updateMapForAllWithValue(null, listId, mEncodedEmail,
                    updateShoppingListData, "", shoppingListMap);
updateShoppingListData.put("/" + Constants.FIREBASE_LOCATION_OWNER_MAPPINGS + "/" + listId, mEncodedEmail);
firebaseRef.updateChildren(updateShoppingListData)
addFriendActivity -2pt


7-25
ShareListActivity -1pt

7-26
ShareListActivity -1pt
FriendAdapter -1pt
FrontEnd -1pt



7-27
Camera take picture and retrieve picture from file -3pt
fragment into3 -1pt
InstagramFragment -1pt
stuck at taking pictures inside fragment -1pt
Solved fragment to start Camera Activity -1pt


7-29
stuck with actionbar -1pt
instagram fragment -1pt
arduino -2pts

7-30
tablayout + image storage with Firebase -1pt
android guide on saving files of taken pictures from camera -1pt
saving pictures to firebase -2pts

7-31
stuck in camera 
Camera bug is not solvable since we are accessing the wrong directory -2pts
Firebase instagram sample - ProfileActivity but i am not using Glide -2pts
succeeded running Firebase Sample instagram -1pt

8-1
Firebase Sample Instagram PostFragment -2pts
RecyclerView, FirebaseRecyclerAdapter, Viewholder review -1pt
FirebasePostQueryAdapter, PostFragment, PostViewHolder -1pt


8-2
FirebaseRecyclerAdapter -1pt
RecyclerView.Adapter -1pt
PostViewHolder -1pt
You should be ashamed that you want local girls.

8-3
ContactFragment -2pts
ContactAdapter -1pt
Finished Contact List Fragment -1pt
Arduino -1pt

Recycler.Adapter<viewHolder> - (List<String> paths, OnSetupViewListener onSetupViewListener, Context context, List<Contact> contacts) parameters are anything related to ACTUAL data + onCreateViewHolder: viewHolder(inflate(layout) + onBindViewHolder: viewHolder.mTextView.setText(mContactsArray.get(position).getName())
i.e. layout only get inflated into ViewHolder when it is called onCreateViewHolder in RecyclerView.Adapter
ViewHolder is just a public constructor that takes a parameter of an inflate(layout) -> findViewById

FirebaseRecyclerAdapter<User.class, viewHolder> - (User.class, layout, viewHolder, ref) + populateViewHolder(viewHolder, postActualDataPopulated, position)  

FirebaseListAdapter<User.class> - (activity, User.class, layout, ref) + populateView

ArrayListAdapter(activity, layout, ArrayList) + populateView


8-4
InstagramFragment CameraActivity -1pt 
Firebase setValue vs updateChildren -1pt
Finish up saving into Firebase -2pts
FirebaseRecyclerAdapter + ViewHolder + onCreatedViewHolder + onBinderViewHolder -1pt

soccer cleats start afresh


8-5

Primitivly implement FirebaseRecyclerAdapter: manually loop through all DatasnapShot to getkey() -> put inan ArrayList<String> -> manually get position add value listener or loop through the list to add  
	
	for (DataSnapshot snapshot : dataSnapshot.getChildren())
	final List<String> postPaths.add(snapshot.getKey());
	DatabaseReference ref = FirebaseUtil.getPostsRef().child(mPostPaths.get(position))
	mOnSetupViewListener.onSetupView(holder, post, holder.getAdapterPosition(),dataSnapshot.getKey());

Chat Activity -2pts
Arduino bluetooth -1pt

8-6
write to firebase of chats -4pts
UI -2pts

8-7
neural nets -2pt
finish up the app -3pts
order arduino
naive bayes classifier movie review just positive and negative -2pts
naive-bayes classifier in python to classify category of walmart from calculating each word's occurence each category description - srinath, facebook, google, cmu -2pts 


8-8
change package name + record demo -4pts





8-10
machine learning sendtex -3pts
	Quandl.get("WIKI/GOOGL")
	df[["Adj. Open", "Adj. High", "Adj. Low", "Adj. Volume"]]
	df["HL_PCT"] = (df["Adj.High"]-df["Adj.Close"])/df["Adj.Open"]*100
	df.fillna(-9999999, inplace=True)

	forecast_col = "Adj. Close"
	forecast_out = int(math.ceil(0.01*len(df)))
	df("label") = df[forecast_col].shift(-forecast_out)
	df.dropna(implace=True)

	# convert Dataframe into Array: X->features array, y->label
	X = np.array(df.drop(['label'],1))
	X = preprocessing.scale(X)
	y = np.array(df(['label']))
	
	# split df into 80% training and 20% testing
	X_train, X_test, y_train, y_test = cross.validation.train_test_split(X, y, test_size=0.2)
	
	# pick classifer: LinearRegression(), svm(kernal=poly)
	classifer = LinearRegression()
	classifer.fit(X_train, y_train)
	accuracy = classifer.score(X_test, y_test)
	print(accuracy)


cs109 probability -1pt

8-11
Neural network basic intuition

Neural nets welch labs -1pt
activation function e.g. sigmoid allows neural nets to model non-linear patterns that simplier model may miss.

Jeff Heaton
Neural network calculation -1pt
Neural network is only a way to calculate the weight given input features and output label

UT neural nets -1pt
Hairbobo sample app -2pts


8-12
Square Error Cost function
Gradient Descent 3pts
Partial derivatives-1pt
Gradient Descent intuition -1pt



8-13
Shame -> 8am sleep, blonde, restart

Matlab 229 assign1 plotData.m -1pt
hypothesis(x) = theta0 + theta1*x -2pts
to find theta0 and theta1 -> you need to minimize cost function by derivatives and lest square method
J(theta0, theta1) = 1/2m sum(h(x)-y)^2 where square to ensure positivity, division by 2 easy to calculate
cost function J(theta0, theta1) calculates the difference between h(x) vs y for each theta0 and theta1
transition from graph h(x) to J(theta) 
J(theta) x axis is theta + it is convex since the difference between h(x) and y increases quadrically when slope theta increases
gradient descent: core idea is differentiate J(theta) and theta to the their rate of change -> 1. if it is big, then means slope or theta is big so much deduction 2. determine positive and negative so that it knows which way to go to reach minimum of J(theta) 

assign1 -2pts
computeCost.m
gradientDescent.m
differentiate in matlab

8-15
finished assign1 -2pts
backpropagation -1pt
<<<<<<< HEAD

8-16
backpropagation intuition - minimize delta (difference between sum of weighted node and actual output)-> apply gradient descent to try different weight until delta minimize
delta= (rate of changes of cost function with respect to one incoming node) i.e. adjust weight of incoming nodes using gradient descent algo until delta is minimized

logistic regression is sigmoid classification
logistic regression 

decision boundaries -1pt


8-18
109 variance -1pt
Error correction algo -1pt 
unique identifier takes at least 2 power more memory for sending the string
if you want to be short then even as check
1 even only needs 2 to flip
2 even same takes 2
however if 3 then could includes all other groups

Random Variable, Expectation, Variance -4pts 
Expectation and variance is so extra: you want to stress there can not be probability in random variable since it goes to infinity, but then you change the probability to frequency to say that there could still be population mean of an random event if you add up all different probabilities of a random event happening

First ask: what is the probability of getting head out of 1 toss
Then ask: now how about probability of getting heads out of 6 tosses
now you are stuck so you invent Random Variable, Expectation and Variance

8-19
106b marty Binary tree and binary search tree lecture review -2pts



8-20
Expected value -3pts 

8-21
Luke Welling Php mysql webdevelopment -2pts
Chapter 9
Accessing Mysql database from web with php e.g. write and query data from database 

Chapter 17
Implementing Authentication in php mysql 
Register username and password
cyto algo to encrypt password 
Basic Authentication using php, .htaccess, mode_auth_mysql


 
8-22 
Session control

https://www.youtube.com/watch?v=q9S51sykd1A
Mysql setup -3pts

Still stuck at using workbench vs phpmydmin
Downloaded older version 5 of workbench
Download XAMPP to set up phpmyadmin
Install Mysql on your mac 


8-23
Poissom 1pt


8-24
106b C++ LinkedList.cpp -1pt

8-25
Nosql vs sql intuition -1pt
nosql store data in memory using primative data strucutre whereas sql has a preset schema that is like a struct. When you install mysql it is like a C program that have preset methods allocate memory. Then when you CREATE TABLE students(first_name VARCHAR(30) NOT NULL, student_id INT UNSIGNED NOT NULL AUTOINCREMENT PRIMARY KEY); it creates a huge struct and if data did not get written into certain table it is wasted. It is like pre-announcing 1000k of memory stick together since they are all related and relevant to my search. i.e. very likely my search is going to need all these memory data together. However for data of very different types and complicated structures no or a customable schema is more suitable. 

cs106b Linkedlist.cpp ListNode.h LinkedListClient.cpp -1pt
pointer is the only way to concatenate memory together as it stores the memory address of the other datatype

BinarySearchTree Intuition -2pts
adds and containsm traverse through the tree

DFS in code C++ intuition -2pts
see http://www.codewithc.com/depth-first-search-in-c/

8-26
KD tree code intuition -1pt
recursion: 

// Inserts a new node and returns root of modified tree
// The parameter depth is used to decide axis of comparison
Node *insertRec(Node *root, int point[], unsigned depth)
{
    // Tree is empty?
    if (root == NULL)
       return newNode(point);
 
    // Calculate current dimension (cd) of comparison
    unsigned cd = depth % k;
 
    // Compare the new point with root on current dimension 'cd'
    // and decide the left or right subtree
    // Node* root is updated to root->left so traverse down the node
    // i.e. root->point[cd] ==> root->left->point[x] //if on x dimension comparison
    // i.e. point[x] < root->left->point[x] // 2,3 vs 12,7 ----> 2 < 12
    if (point[cd] < (root->point[cd]))
        root->left  = insertRec(root->left, point, depth + 1);
    else
        root->right = insertRec(root->right, point, depth + 1);
 
    return root;
}	

SEE: http://www.geeksforgeeks.org/k-dimensional-tree/




BFS code intuition -2pts
DFS - use stack + keep a visited array 
BFS - use queue + visited array 

// INTUITION: rear->next ==> 002->next = 003 then 002->next(i.e.003)->next = 004 etc. as long as you have an updated rear
void Queue::enqueue(int data)
{
    node *temp = new node();
    temp->info = data;
    temp->next = NULL;
    if(front == NULL){
        front = temp;
    }else{
        rear->next = temp;
    }
    rear = temp;
}

// INTUITION: front->next means goes to address 010, dereference it to get the actual node, and then get the Node* next which is the next node's address
// therefore when we first enqueue: rear->next = front->next when enqueue 
// e.g. when first enqueue 0x010 front = 0x010 rear =0x010: rear->next means goes 0x010, dereference to write next address i.e. 0x020
// when dequeue front->next = 0x010 deference, get next, 0x020
int Queue::dequeue() {
    node *temp = new node();
    int value;
    if(front == NULL){
        cout<<"\nQueue is Emtpty\n";
    }else{
        temp = front;
        value = temp->info;
        front = front->next;
        delete temp;
    }
    return value;
}

SEE: http://www.codewithc.com/breadth-first-search-in-c/

8-27
Tutorial on hashing for look up 
Hashing into array uses key itself as an address into the data structure, convert key into address through integer key -> guarantee O(1) -> whereas BST due to its comparison tree data structure -> guarantee O(logN) for searching

The idea for hashing is that it is super easy for searching and look up -> but no concept for add and delete + clash of address collision + convert string into table address

Applications:
Compiler, a hash table will likely be used for keyword and identifier storage because a compiler needs quick access to this information + optimizing switch statements 
// what is keyword and identifier storage???

Implementation of a cache, and many web browsers and operating systems will use a hash table for just that.

Cryptography, where algorithms that are geared more toward security are used to create digital fingerpints for authentication and data integrity.

The quality of hashing depends on probability it generates
e.g. add-shift-xor produces 2^11*n combination where n is number of char in string
Normal 8 passwords combination produces 88^8 --> e15 combination
e15: shift to the left 15 digits or 1 times 10 to the power of 15

see: https://www.lamission.edu/learningcenter/docs/mathlab/Symbols_Notations/what%20does%20E%20mean%20on%20a%20calculator.pdf

bool OurHashMap::containsValue(string key){
	int bucketIndex = hashMap(key) % numBuckets;
	// forloop run through the bucket 
};

Behind MongoDB is a giant hash algorithm 
nosql they just hash data into a memory address and therefore super quick look up but the data stored in each bucket /cell related to each other in meaningless way and so unless one defines data schema beforehand we could not get relational or query feature

Distributed evenly: shifting bits and xor operations does not create a pattern + hit every numbers after hash

See: http://www.eternallyconfuzzled.com/tuts/algorithms/jsw_tut_hashing.aspx
 
Hashmap code -keith intuition + rehash intuition -2pts



8-28
Qt creator creating class and object debugging - 2pts

8-29
tutorial
library card research paper 

START OF OPERATING SYSTEM MARATHON
operating system intuition -3pts

file system
it is like creating file objects with meta data of memory address, directories and files it contains

path 
when you cd the path, you are traversing the nodes of memory addresses

io
50% system crashes come from device drivers

memory mapped i/o
maps each device's control registers to a range of physical address on memory bus, meaning reading and writing memory by CPU does not go to main memory but directly to registers on I/O devices' controller

clock algorithm
requesting pages when memory is low

kernel 
protect certain processes
part written in assembly, part written in c

DMA 
DMA transfer between DRAM(main memory) and SSD(secondary memory) 

Most fundamental system calls has to be written in assembly

8-30

Kernel programming interface/ system call interface/ functional interface -4pts
functionality operating system provides to users application program

Process managment
concurrency, mutlitasking, creating a new instance of itself
User-level process management e.g. C compiler shell commnad line interpreter -> compile c files into objects files and linked all object files into executable with multiple processes

Thread management

I/O

Memory management

Design/where to put operating system functionalty/system call interface
1. user level program e.g. login
2. user level library linked in application e.g. user interface widgets
3. system call
4. system call in standalone server processes invoked by kernel


Examples fo system call:
fork()
exec()


Inode:
Inode store meta of a directory or file 
Most important part of metadata is block of memory address of its content/files it contains
Each file/directory has its inode/metadata

Set up pintos environment -2pts
Question i still dont understand: why pintos could make + pintos run-multiple alarm but mac could not even make????????????????????????????



e.g.
when ls -l in home directory
1. HomeDirectory inode -> contains the block of memory that stores all files names + inode memory address
2. if its is file -> then inode contains block of memory of its content
3. if its is folder -> follow inode to memory address -> get all directory content + each inode get metadata






8-31
basic-fork.c -1pt
grandparents pid always 21130
parent's pid always random
childs pid always random + 1
when parent finishes before child: child getppid(): 1, if parents not yet finish getppid returns parent's pid
child pid_t/return value is always 0 

thread.c -1pt
what is nice value + mlfqs?????????
what is page?????
what is thread's kernel stack vs struct thread ???
why are they sharing page??
what is a magic member of running thread's 'struct thread' is set to THREAD_MAGIC???? 
why when kernel stack or struct thread grows too large, causing stackoverflow would change the value of THREAD_MAGIC??? 
what is actual priority after donation??
what is absolute ticks to wake up current threads

loader.S -1pt
16-bit segment

Finished reading paper about compiler does not work well with multithreading in C -2pts



9-1
cs162 Introduction to processes -1pt

cs140 reading through assignment 1 - 2pts
interrupt.c
switch.S
palloc.c



9-2
Maryland: stack push and poping -2pts 
see: https://www.cs.umd.edu/class/sum2003/cmsc311/Notes/Mips/stack.html

RISC - reduced instruction set computing
CPU design a simplified instruction set provides higher performance when combined with microprocessor cycles per instruction 

Directly-mapped cache

cs106b LinkedList implementation -1pt
Arraylist.cpp -3pts

9-3
LinkedList implementation -2pts
cs106l intro -1pt

9-4
stanford library -1pt
stream hello world print table -2pts
106l stream-manupulator 1pt


C++ is an abstraction of C, C is an abstraction of assembly and registers
stream is saving data in buffer/cache
stringstream vs fstream vs cin vs iostream -> differs in the memory space they allocate to store
database has schema and null -> since it is implemented in C++ using stream to read/write data


Abstraction is simplication of complex idea, both in software and real life. Allows you to use it without understanding its underlying mechanism. 

ch3 + 4 cs106l course reader, C++ library method and memory and hardware -2pts


9-5 
snake implementation 
milestone 1 set up struct + reading map from level.txt to vector<string> board 

milestone 2 calculate snake nextNode using rand() + check if nextNode == kEmptyTile in Board[nextNode.row][col]
snake.push_front()
snake.pop_back()
snake.front()
snake.back()

milestone 3 update board + print board 
board[snake.row][snake.col] = kSnakeTile
for i<board.size() cout<<board[i]<<endl


9-6 
done snake most primative implementation 8pts
reading interview questions 

selection sort code implementation -2pts
select the smallest

stuck at trying to implement selection sort but implemented bubble sort 
int i = j ****************** (i spent 2 hours debugging) 

insertion sort -1pt
insert into sorted array
hardest part is implementation of sorted array -> add to the last -> keep swap if bigger previous term

9-7
mergeSort code intuition -1pt
mergeSort milestone 1 recursively divide up array + stack sequence of recursion 1pt
mergeSort code implementation 2pts
hardest part is the merge when i found out there could be a out of bound situation + worrying about stack sequence of running function and appending ->figure out the stack sequence by cout ->  figuring out how to compare them when there could be an out of bounds situation -> the more straightforward way is to say there are 2 situations: 1. both stay in bounds then normal compare lefthead/righthead +1   2. anyone goes out of bounds then just append the rest  

linear algebra review -1pt
1. linear algebra/matrix is structured in this way -> because matrix is used for huge data sets 
Two huge data sets there are some relationship between them
human more inclined to read data horizontally + varaibles are only one row thick so they columnized it 

2. matrix is another way of saying 2d array
matrix is graphical representation of classification problem
linear algebra is calculation of classification problem

3. notation
scalar multiplication
vector -> n x 1 matrix

linear regression + gradient descent review 


9-8 
logistic regression - decision boundary -1pt
logistic regression -> find decision boundary so to classify data sets into positive and negative -> as long as we could tell one set of data is positive and one set of data is negative then just symbolicly apply sigmoid to convert them into 0.5-1.0 and give intensity/value to probability
Sigmoid function itself is just for scaling -> BUT DECISION BOUNDARY IS EVERY THING as they directly tell whether a dataset is positive
neural network + logistic regression intuition -4pts

9-9
neural network intuition -2pts
backward propagation intuition

9-10
in logistic regression, gradient descent is decreasing partial derivative of cost BUT NOT the cost itself. for a linear classifier, the fact that it is in +ve or -ve area affects gradient decsnet despite the absolute value of cost being the same. i.e. if in the +ve area, as theta increase the cost increase exponestially so cost is +ve while if in -ve area, as theta increase cost decrease so cost is negative -> -ve add theta, +ve minus theta

9-11
WASTED AN ENTIRE DAY with a white girl that is not American.

9-12
predict.m 1pt
ex3 multiclass logistic regression number recognition -2pts
ex3 neural net forward algorithm 1pt


9-13
kernel svm intuition 2pts
SVM with gaussian kernel vs logstic regression -1pt
clustering algorithm -1pt
1. closest trainging example assignment - after random initializing k number of clusters, for each training example, assign the closest cluster -> if a cluster ended up not having any closest point assigned, delete the cluster
2. move cluster to the mean of training examples - for all the assigned training example, calculate the mean and then move cluster to the mean, repeat 1 


106a 
What is inheritance? 
subclass extends superclass or children inherits all functions and fields of parent
copying existing class taking its core attributes and behaviors + add own behavior
e.g. GObject is parent of all children GRect, GLabel, GPolygon

Class Object forms the root of overall inheritance tree of all Java classes 
GObject implicitly extends Object without saying extends

return super.getsalary() -> refers to superclass's method getsalary() when replacing it in inheritance


9-14,15,16,17,18,19
3 + 7 + 5 + 5 + 5 + 5 pts

sudden sensetime web development job interview 11pts

jquery - manipulate html
change tag action
$('div id').toggle()
change html
$('#button id').html('I am changing the html from using javascript library jquery')

event binding
javascript event name -> 'click', 'mouseover'
$('#button id').on('click', function(){
	$('#dividorbuttonid').find('div class').html // same as $('#divid .divclass').html('mynewcontent')
})

smarter better event binding listening to all button at once





ajax

php mysql
phpmyadmin or mysqlworkbench is gui for mysql
milestone 1: use your own computer as server to set up php mysql







Nodejs
why heroku when ams present:
heroku platform as a service: environment where you could just push code and do minimal configuration, less to build or maintain e.g. load balancer/caching layer Varnish, server like apache, passenger, nginx to serve your code, deploy configure clustered database instance like PostgreSQL, deployment system like Capistrano, also log aggregation  

see: http://stackoverflow.com/questions/9802259/why-do-people-use-heroku-when-aws-is-present-whats-distinguishing-about-heroku

milestone 1: mongodb, http server, connecting backend to frontend
since i have installed mongodb through homebrew instead of tar the zip
now i have to call mongod --dbpath /usr/local/var/mongodb/ to start mongodb with db path /usr/loca/var/mongodb
also you need to start mongod before calling mongo shell 
see: http://stackoverflow.com/questions/28771558/set-dbpath-in-mongodb-homebrew-installed-mac-os
see: https://courses.edx.org/courses/course-v1:MongoDBx+M101x+2T2016/courseware/4cf555e7a26d4fbb87503adf07ef03ce/4f0c609a29a842029b0ac2ffee7d8f5b/

db.test.insert({key:"value"}; // insert into collection test
db.test.findOne{key:"value"}); // return objectID 12 bytes hash code
db // return test the default collection

package.json decribes your app's meta data e.g. name, version, dependencies
Underscore: functional programming inspired utility function for javascript

npm install

mongodb nodejs driver

mongodb insert
1. mongodb = require('mongodb');
2. uri = 'mongodb://localhost:27010/example';
3. mongodb.MongoClient.connect(uri, function(error, db){db.collection('sample').insert({key:"value"}, function(error, result){db.collection('sample'.find().toArray(function(error, docs){docs.forEach(function(doc){console.log()JSON.stringify(doc)}}}; process.exit(0);

mongoose to interface with mongodb
mongoose -> object document mapper ODM that provides functionality like schema validation

query

callback = asychronous 
asych and promises libraries 
javascript is running a loop of event handler that could register event handling
single threaded + loop to handle waiting time of network io, once network io is finished, gets added to the queue of event being executed
e.g. mongoclient.connect does not block the event loop while it connects -> but it registers an event handler that tells nodejs to execute callback function once network io connecting to mongod server is done

java, c++ -> code executes in the current iteration and thus block the event loop, only 1 loop -> sychronous 
js -> code that registers event handler and thus not blocking the event loop, e.g. file io, network io is js are typically asychronous, callback means asychronous as it is registering event handler -> this enables nodejs to be highly concurrent by default as no need to worry about multiple thread 
callback is handy to determine your code doing too much io


5 hours punishment

require() -> nodejs way to break large code into small manageable files, preferred way to share code with other file
module.exports -> the return value when require() is called 

// -g -> --grep globally search for regular expression and print
// only run fails test but not the others.
./node_modules/.bin/mocha -g "fail" test.js

npm shortcut helper
package.json -> "scripts": {"test": "mocha test.js", "test-kitten" : "mocha -R nyan test.js"}
npm test // runs the test script defined in package.json
npm run test-kitten

// install mocha globally <- not preferred as breaks all node projects on the machien, prefer to install local dependencies
npm install mocha -g 

./node_modules/.bin/gulp test
npm run watch

nodejs: validating client's data, authentication, routing, receiving http from angularjs for JSON





MONGODB

schema: set of rules of what fields + fields properties to consider valid input

reading a single MongoDB document requires fewer different non-sequential hard drive read

indexing
db.users.createIndex({name:1}); // create a linked list using value of name, same for multi-key index





NodeJS - building the RESTful api
connecting routing with database -> listening for request from routing + send out response with data from Mongo

translate get post put delete http request to create read update delete database command

app.set vs app.get vs app.use vs app.post

controller: connect app.get or app.post to mongoose



AngularJS
$scope variable + method
sending http post/get to nodejs api
manipulate html 
add javscript variable to html

2 way data binding: bind the state of a javascript variable to the state of html


<span> provides a way to add a hook to a part of a text or a part of a document and grouping inline documents together
<div> and <p> starts a new line
<hr> stupid line
<h1> bigger header style + seperate line
<h2> smaller than h1 header style + separate line
global attribute class is for css


console
$('button[data-panel=panel1]') // looking for button with attribute  
$('button[id=btn1]') //id is an attribute -> return <button id="btn1" data="data1">button1</button> 


# 1) Install heroku and git
$ sudo apt-get install -y git-core
$ wget -qO- https://toolbelt.heroku.com/install-ubuntu.sh | sh
$ which git
$ which heroku
sudo apt-get remove git-core // when you want to remove

# 2) Login and set up your SSH keys
$ heroku login
$ ssh-keygen -t rsa
$ heroku keys:add

3) Clone a sample repo and push to heroku
$ git clone https://github.com/heroku/node-js-sample.git
$ cd node-js-sample
$ heroku create
$ git push heroku master

// looking for pdf preview -> then kill it
ps xw | grep Preview

ls -alrth myfile # list metadata on myfile
source ~/.bash_profile // refresh changes made to alias
echo "line1" >> myfile # append via ’>>’ to a file
rm -i myfile2
cp -av /home/ubuntu/techblog /home/ubuntu/techblog2
rmdir mydir # won’t work because there’s a file in there
rm -rf mydir # VERY dangerous command, use with caution
rm myfile*
mv git bin // move git into bin directory
mv git git.txt // change name
wget https://spark-public.s3.amazonaws.com/startup/code/git
export PATH=$HOME/bin:$PATH // append git in $HOME/bin to exisitng $PATH 
which -a git // show use all the git binary

// copy hello.txt into ubuntu server
scp -i skey.pem hello.txt ubuntu@ec2-54-218-73-84.us-west-2.compute.amazonaws.com:~/

scp -i stevealbertwong.pem helloworld.c ubuntu@ec2-54-191-202-37.us-west-2.compute.amazonaws.com:~/

wget https://spark-public.s3.amazonaws.com/startup/code/simple.sh
chmod u+x simple.sh // set to executable
chmod 777 simple.sh // set to executable same
./simple.sh


sudo apt-get install -y make gcc // install gcc and make

// move from your computer to remote aws
1 # -- Execute on local computer
2 # Copy hello.txt from local computer to remote home directory
3 $ scp -i skey.pem hello.txt ubuntu@ec2-54-218-73-84.us-west-2.compute.amazonaws.com:~/
4
5 # Copy h.txt from local to remote home directory, renaming it foo.txt
6 $ scp -i skey.pem h.txt ubuntu@ec2-54-218-73-84.us-west-2.compute.amazonaws.com:~/foo.txt
7
8 # Copying ~/foo.txt from remote to current local directory
9 $ scp -i skey.pem ubuntu@ec2-54-218-73-84.us-west-2.compute.amazonaws.com:~/foo.txt .
10
11 # Copying ~/foo.txt from remote to local directory cc, renaming it a.b
12 $ scp -i skey.pem ubuntu@ec2-54-218-73-84.us-west-2.compute.amazonaws.com:~/foo.txt cc/a.b


// setup nodejs npm 
1 $ sudo apt-get update
2 # Install a special package
3 $ sudo apt-get install -y python-software-properties python g++ make
4 # Add a new repository for apt-get to search
5 $ sudo add-apt-repository ppa:chris-lea/node.js
6 # Update apt-get’s knowledge of which packages are where
7 $ sudo apt-get update
8 # Now install nodejs and npm
9 $ sudo apt-get install -y nodejs

"Emacs is an excellent operating system. All it's missing is a decent text editor."

server configuration(Paas) computer configuration(Iaas) + code = Google api (Saas)


MEAN chapter 20 AngularJS

// html rootelement is angularjs code's name + angularJS knows where to compile the code
<html ng-app="AngularModulenName">
var app = angular.module("AngularModuleName", [])





// http.createServer(app).listen(process.env.PORT);
// mongoose.connect(process.env.MONGOLAB_URI || "mongo://localhost:27017/test")


// godaddy destinated forwarding ip
50.63.202.1

npm install nodemon
nodemon node_server.js

token is just a hashed username on the client/broswer so hacker could not just easily use username for header authentication to get/post to database
finished all basic authentication + switch pages + login logout + token cookie save token into database + db.remove(_id, userid)


4 days hackathon failed miserably: tech blog. I got a F


9-20 -3pts
React js - instagram
ember.js -Yahoo, groupon
Jquery + Ajax
ink
D3
Modernizr
CloudFlare cdn
Semantic-ui
Raphael-js graphics -linkedin
Hogan.js -twitter
YUI.js -linkedin
Varnish-cache tool
Twitter emoji
backbone.js - Airbnb, Pinterest
meteor.js
moment.js
Mustache.js
Spin.js
underscore.js
video.js
reCaptcha
MathJax.js
Mediaelement.js
XRegExp.js
head.js
Prototype.js
swfobject
require.js
JqueryUI
fontawesome
Sweetalert
handlebars.js-amazon
sumome
highchart js plugin 
draft.js - facebook code editor, https://github.com/facebook/draft-js
immutable.js
TweenMax.js - Vote for america by dutch world map showing votes figure
https://www.npmjs.com/package/rainbow-code

less interesting
Vue.js
Mercury.js
Ploymer.js
Aurelia.js


Advertisement network
adroll
google ad sense 
doubleclick ad exchange
adzerk
Appnexus
buysellad
carbonad


Analytics
new relic

disqus comment system




AngularJS directives
the idea of js is programming html tag 
the idea of angular js is programming html tag by turning html tag in object -> directives

https://docs.angularjs.org/api/ng/directive/ngPluralize
ngPluralize - steven and cara and 10 other people have liked -> like if $scope.count = 1, display certain statement

https://docs.angularjs.org/api/ng/input/input%5Bemail%5D
form validation -email

https://docs.angularjs.org/api/ng/directive/ngInit
ngInit + ngRepeat - ngInit could store $index of for-loop

https://docs.angularjs.org/api/ng/service/$cacheFactory
for loop to get both key and value 
  <div ng-repeat="(key, value) in cache.info()">
    <span ng-bind="key"></span>
    <span>: </span>
    <b ng-bind="value"></b>

https://docs.angularjs.org/api/ng/service/$filter
$filter
$scope.filteredText = $filter('uppercase')($scope.originalText);

<span ngBind="name"> </span> -> {{name}}


Browsify
Node.js JavaScript, also known as CommonJS,
with its synchronous require calls and module
to exports/require("express") property, won't work in the browser as is.


Best quote of today:
if you dont know how to parse a website then you should probably not use beautiful soup

pythonanywhere
linode
digital ocean


9-21,22,23 6, 7, 4pts
flask - html logic, same template to show highchartjs through manipulating varables
apache + nginx
mysqladmin

200
301 - move permanetly, redirect
304 - not moodified, cached
400 - bad request, bad login credential, not found in database
404 - page not found, no such route
405 - method not allowed/found, <form action="/template" method="post"> but in flask.py there is no @app.route("/template", methods=['GET', 'POST']) def template(): if request.method="POST": return render_template("login.html") 
500 - infamous internal server error


flask is like combining nodejs and angularjs 
MEAN stack could call different servers using the same code

wsgi is between flask(gateway) and nginx(server)
virtualenv is to create python environment sandbox -> multiple version of python with multiple modules where each environment isolated from each other 

python-> static typing : indent without {} to indicate scope
flask jinja template: {% for p in paragraph%} {{p}} {% endfor %}

template engine is literally a compiler that replace {% if pageType == 'about '%} {%  endif %} with right tag and just send those tags out 

contentmanagement.py -> initiates a dictionary of road map of pages  -> then import into home page 
for dynamic linking for a series of website -> for loop

flask-> the only way to do data binding is render_template("login.html", error=error)

python as scripting language -> stop running the remaining script when it hits return

p means password
run ./mysql -u root -p
mysql --user=root -p 

for mysql basic command walkthrough
see: http://www.newthinktank.com/2014/08/mysql-video-tutorial/

see: sentdex flask web
https://www.youtube.com/watch?v=T5eJeCoprCA&index=13&list=PLQVvvaa0QuDc_owjTbIY4rbgXOFkUYOUB
SHOW DATABASES;
CREATE DATABASE pythonprogramming;
CREATE TABLE users;
show databases;
select database; //show the current database
drop database if exists test3; // delete database
use test2; // select database test2
show tables; // different student and table
describe student;

CREATE TABLE student(first_name VARCHAR(30) NOT NULL, last_name VARCHAR(30) NOT NULL, email VARCHAR(60) NULL, street VARCHAR(50) NOT NULL, city VARCHAR(40) NOT NULL, state CHAR(2) NOT NULL DEFAULT "PA", zip MEDIUMINT UNSIGNED NOT NULL, phone VARCHAR(20) NOT NULL, birth_date DATE NOT NULL, sex ENUM('M', 'F') NOT NULL, date_entered TIMESTAMP, lunch_cost FLOAT NULL, student_id INT UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY);

INSERT INTO student VALUES('Andy', 'Brennan', 'abrennan@aol.com', '281 4th St', 'Jacksonville', 'NC', 28540, '792-223-8902', "1960-12-27",'M', NOW(), 3.50, NULL);


anaconda command
http://conda.pydata.org/docs/using/pkgs.html
conda list
conda search beautiful-soup
conda info --envs
conda install flask-wtf


ipython notebook --notebook-dir='~/Desktop/Stanford/CS145-Database/lecture-5-7' // or just cd to lecture-5-7 then ipython notebook 

sql injection: 
x = c.execute('SELECT * FROM users WHERE username = (%s)', username)

' or '1'='1
password:'or1=1--
' or '1'='1 and firstname <> 'Mickey // and firstname is not equal to Mickey
' or '1'='1 and firstname <> 'Mickey' and firstname <>'Donald

# *arg:list of arguments **kwarg-> key value arguments both-> list of key value argument
def function(*args, **kwargs):
    for arg in 
	for item in kwargs.items():
	    print item

funtion(x=10, v=1290, 12=sadasj, 9=sdjao) 


9-24, 25 4 + 4pts
MEAN stack full on coding
finished login page

mongod --dbpath /usr/local/var/mongodb/

// cannot really 'delete' a cookie per se but you could set it expired 
document.cookie = 'nameofthecookie' + '=;expires=Thu, 05 Oct 1990 00:00:01 GMT;';

session
store a connection between server and client
e.g. req.session.errors = errors // store errors of form validation from post
e.g. req.session.success = true // store meta data of req, res fo a connection

CSS
element>element	div > p	Selects all <p> elements where the parent is a <div> element

mongodb update
SEE: https://docs.mongodb.com/manual/reference/method/db.collection.update/#db.collection.update
db.users.update({username:'steven'}, {$set:{password:"steven"}})




CHROME CONSOLE HACKING TRAINING
command + option + c to bring up the console


mongoose find query
Person.
  find({
    occupation: /host/,
    'name.last': 'Ghost',
    age: { $gt: 17, $lt: 66 },
    likes: { $in: ['vaporizing', 'talking'] }
  }).
  limit(10).
  sort({ occupation: -1 }).
  select({ name: 1, occupation: 1 }).
  exec(callback);


TECHBLOG TODO
1. email verification
2. nav bar
3. listing and linking of topics
4. comment
5. picture


9-26 2pts
smart search algorithm
https://en.wikipedia.org/wiki/Rocchio_algorithm

rugby + interview

intuition of naive bayes 
P(cs106 | java) = P(java | cs106) x P(cs106) / P(java) 
i.e. frequency of cs106 in the world of java = frequency of java in cs106 / frequncy of java overall in cs106, 108, 109

i.e. naive bayes is just explaining difference between p(java|cs106)p(cs106) and p(java)

why naive bayes
when search a word of all articles, instead of returning all articles that contain the word, we return the article with highest frequency of the word

a contrast between neural net(most significant/distinguishing word that represents cs106) and naive bayes(frequency)
but maybe they are actually calculating the same thing
as neural net with a hidden layer, most distinguishing factors are unique word (i.e. most frequent highest probability word from that article as well) or most frequent words
the difference is neural net is to distinguish the most distinguishing factor while naive bayes can only tell you how frequent it is so you could only hope for you stumble upon the most distinguished one



9-27

https://www.youtube.com/watch?v=iX5V1WpxxkY
RNN intuition -3pts
machine-learning paper reading

linked list creates an object data structure
the idea of object comes because you could store memory address

The idea of SVM is to construct a hyperplane
to seperate the two classes of data with the gap being as
wide as possible.

python connecting to mysql - google app engine

db = MySQLdb.connect(unix_socket='/cloudsql/' + _INSTANCE_NAME, db='guestbook', user='myuser')

cursor = db.cursor()
cursor.execute('SELECT guestname, content, entryID FROM entries')
guestlist = [];
for row in cursor.fetchall()
	guestlist.append(cgi.escape(row[0]), cgi.escape(row[1]), cgi.escape(row[2]))
db.close()

9-28 
6 hours spent on job offer

see: http://www.existor.com/en/ml-rnn.html
see: http://karpathy.github.io/2015/05/21/rnn-effectiveness/
see: http://cs.stanford.edu/people/karpathy/
see: http://cs231n.github.io/neural-networks-case-study/
see: http://cs231n.github.io/linear-classify/


RNN intuition and code intuition -2pts -> from now you could manipulate neurons
i disagree that feed forward network could never do recurrent neural network does
i do agree that vanilla neural network is designed more to learn a relationship between the current input and output
and the existence of RNN manipulates and designates a set of neurons to store the sequence of input -> therefore extra connections to teach it about sequences in the data


9-29 -5pts
softmax - force the output to represent a probability distribution across discrete alternative
neural network matlab
linear neuron
public key vs private key

http://cs231n.github.io/neural-networks-case-study/
each image in CIFAR-10 is a point in 3072-dimensional space (32x32x3 pixels)
i.e. 32*32*3= 3072 an equation with 3072 weights/parameters, and 1 point carries all 3072 values of paramters

We are going to measure our unhappiness with outcomes such as this one with a loss function (or sometimes also referred to as the cost function or the objective)

softmax function takes a vector of arbitrary real-valued scores (in zz) and squashes it to a vector of values between zero and one that sum to one.

https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence
KL-divergence from Q to P D(P||Q)
measure of information gained between prior probability distribution Q and posterior/ true probability distribution P

cross entropy = KL divergence

1. matrix multiply 
2. base number + normalize -> i.e. softmax squashes into 0-1 sums up to 1
3. KL divergence from Q to P -> i.e. P/true number = -log(p) Q/raw vector = 1 (normalize always sum up to 1 -> 1

cross-entropy objective wants the predicted distribution to have all of its mass on the correct answer.


softmax + cross entropy = loss function

9-30 -4pts
4-step approach
1. algo intuition (hardest as refresh)
2. algo write out
3. code intuition
4. code write out (hardest as debug + combining 2,3)


cs229 lecture notes
Overview of different learning models

supervised learning = learning algorithms that model p(y|x; θ) probability of y given x, parameterized by θ

logistic regression modeled p(y|x; θ) as hθ(x) = g(θT x) where g is the sigmoid function


Twitter wiki words association
Naive bayes
http://machinelearningmastery.com/naive-bayes-classifier-scratch-python/

python attack apple server to reserve iphone


10-1 5pts
word2vec is very complicated naive bayes calculating co-occurences of all words to each other

one hot encoded vector -> unit basis vectors

bags of words model + skip-gram model (softmax + cross entropy)
CBOW = neural network, predict a word based on context (words in the window)
you need to implement skip-gram

only 1 column in matrix is relevant to 1 word
more hidden nodes within the hidden layers means ???



huffman tree to represent the frequency of word + represents in binary


see: https://buss_jan.gitbooks.io/word2vec/content/chapter2.html

A word is the basic unit of discrete data, defined to be an item from a vocabulary indexed by 1,...,V. We represent words using unit-basis vectors that have a single component equal to one and all other components equal to zero (one-hot encoding). Thus, using superscripts to denote components, the vth word in the vocabulary is represented by a V−vector w such that wv=1 and wu=0 for u≠v.

A document is a sequence of N words denoted by w = (w1,w2,...,wN), where wn is the nth word in the sequence.

A corpus is a collection of M documents denoted by D=w1,w2,...,wM.

A word embedding W:words−>ℝn is a parameterized function mapping words in some language to high-dimensional vectors (perhaps 200 to 500 dimensions). For example, we might find:
W("cat")=(0.2,−0.4,0.7,...)
W("mat")=(0.0,0.6,−0.1,...)


singular value decomposition

virtualenv .env                  # Create a virtual environment
source .env/bin/activate         # Activate the virtual environment
pip install -r requirements.txt  # Install dependencies
# Work on the assignment for a while ...
deactivate                       # Exit the virtual environment


10-2
tree = recursion to do search, if elif elif manually to build tree
parse tree - hierarchical structure of a simple sentence

parse tree to represent mathematical representation

euler's number is the base of natural logarithm-> sigmoid
euler number is chosen for sigmoid since it is the definition of natural logarithm's base number, or logarithm to base e
2.71828
infinity of compound interest
irrational -> not a ratio of integers
transcendental -> not a root of non-zero polynomial with rational coefficients
conclusion for non-math people -> it is chosen because it gives you 0-1


encoding 
https://en.wikipedia.org/wiki/Comparison_of_Unicode_encodings
utf-8(chinese, japaness), latin1(english only), ISO-8859-1(czech)
http://stackoverflow.com/questions/6539881/python-converting-from-iso-8859-1-latin1-to-utf-8
http://balusc.omnifaces.org/2009/05/unicode-how-to-get-characters-right.html


bag of word representation = 1 on vector



10-3
how to convert to low-dimentional representation ??????? using PCA ????????
sliding window generating context vectors?????
what to do after loss -> partial derviative of loss with respect to W with stockhastic gradient descent?????
how to connect output layer nodes to hidden layer in a huffman tree way?????????
negative sampling -> not updating all negative output nodes but just do a sample
negative sampling and huffman tree is for efficiency of updating the output weight matrices


skip-gram vs CBOW different ways of generating the contexts, inputs and expected outputs to neural network
skip-gram -> negative sampling?????????
it takes the predicted word as input layer and context as output


word2vec is doing matrix factorization of co-occurence
i.e. co-occurence matrix = input matrices * output matrices transpose
skip-gram model with negative sampling -> basically doing matrix factorization where the matrix is pointwise mutual information (i.e. point wise co-occurence)
i.e. a carefully constructed matrix factorization = word2vec or wordembedding in general
word embeddings of equal quality to word2vec can be obtained by singular value decomposition

each point in weight matrices -> excitement/inhibit level, thoery, template, frequncey/occurence


evaluate results:
wordsim353, classification, clustering

hidden layer dimention is set to 500-1000 after a point, will not significantly affect result, unless significantly more data from 1gb to 1tb





10-6
end to end memory network
memory module + controller module
soft attention -> train a state vector that tells which sentence to pay more attention to when asnwering that particular question
transform input(question asked) into weight * memory(relevant sentence with weight)
this is actually exactly what i was planning to do
when customer asked a question, i looked for the relevant answers/prepared relevant answers -> then use these relevant answers as inputs to get the label


10-7,8
5pts, 6pts
reading end to end memory network code

sed -n '$=' filename -> counting number of lines

Distance.c - calculate cosine similarity 



Preparation:
GloVe vs word to vec
math notation
224d lecture + assign1
python and c code walk through and play around

googlenews.bn word2vec negative sampling datasets see: https://github.com/3Top/word2vec-api
freebase googlenews.bn word2vec skip-gram
wikitext cut short 

why cosine similarity? + 2 words/phrases + negative sampling(skipgram) + huffman tree (cbow) + reduce to 2d project down high dimentional vector into lower space TSNE/PCA  + mapping entity to vector space to show similar entity cluster really well together
specifying negative words to not included in the search 
singular value decomposition + partial derivatives
stockahastic gradient descent
dynamic logistic regression


top 2 dimentions corresponding to top 2 singular values
low dimentional vector -> singular value decomposition of coocurence matrix 
dense vector representation for every word

SVD computational expensive
instead of SVD -> directly learn low-dimentional word vectors

word2vec is SVD
the output vector is window-based co-occurence matrix summed together in a colum n (for its probability)
as an improvement to SVD as easily incorporate new sentence(window)/document or add word to vocab -> does it in a very online way

optimize cost/objection function -> maximize/minimize

the derivatives of gradients to backprog is too computational expensive 
approximate normalization constant or define negative prediction

syntactic and semantic word analogy that happens to fall out when you compress lower dimentional vector
by just doing vector subtration in embedding space
-> apple and apples -> word vectors difference is just several, a couple of, many etc -> so car cars difference should also be the same with the presense of several, couple of, many etc
e.g. shirt - clothing = chair - furniture
king - man = queen - woman


Problems of discrete representation
wordnet(taxonomy) -> nltk.corpus -> hypernyms e.g. carnivore -> mammal -> panda
synonym sets (good)-> does not give you the exact context + missing hyped badass new words + labour to maintain + other language not well maintained
knowledgebase -> brand: ford, mecedes, fox ...

Dimentionality
20k speech
50k PTB
500k big vocab
13M Google 1T
sparse representation -> capture only that index
only problem of 1 hot is awesome and good have no connection -> cannot tell similarity between vectors

 
Bag of words representation + logistic regression -> spam classifier 

using statistics to define meaning
e.g. politics, love, donald trump
" you shall know a person or a word by the company it keeps and neighbour it has"
represent a word by means of its neighbours

co-occurence matrix of a large text corpus vs word to vec
latent semantic analysis -> word document co-occurence matrix -> capture context or words representing general topic of the document
word to vec -> word window co-occurence matrix-> specific syntactic POS and semantic information
co-occurence matrix provides visualization of window based word to vec

text corpus = wiki datasets
word vectors = weight matrix as each word would become a vector representing probabilities after training

common crawl -> all web pages for 7 years
800b tokens


Count-based
LSA, HAL (lund & burgess)
COALS (Rohde)
Hellinger-PCA (lebret & Collobert)

Direct-prediction
NNLM
HLBL
RNN
Skip-gram/Cbow




Other models:
LSTM
recurrent neural network -> bi-directional, deep
convolutional neural network
end to end memory network
dynamic memory network -> attention mechanism + recurrent neural network-> named entity recognition

LSA / LSI
see: http://stats.stackexchange.com/questions/4735/what-are-the-differences-among-latent-semantic-analysis-lsa-latent-semantic-i

LSA and LSI are mostly used synonymously, with the information is retrieval community usually referring to it as LSI. LSA/LSI uses SVD to decompose the term-document matrix A into a term-concept matrix U, a singular value matrix S, and a concept-document matrix V in the form: A = USV'

HAL - Hyperspace Analogue to Language
sifts through text keeping track of preceding and subsequent contexts
Vectors are extracted from these (often weighted) co-occurrence matrices and specific words are selected to index the semantic space. In many ways I'm given to understand it performs as well as LSA without requiring the mathematically/conceptually complex step of SVD. See Lund & Burgess, 1996 for details.







10-9 4pts
backprog python implementation
error/cost/objective function -> the answer you do crazy matrix multiplication for 

10-10 4pts

entire day on neuralnetwork-simple.py
rugby

chatbot:
retrieval based model with predefined resposes e.g. i am fine
pick response based on input and context
rule based expression matching
hueristic

classifier -> intent of questions

harder -> generative model
neural network on film dialogue + IT support to have domain knowledge
open domain -> talk about any thing -> does not exist yet
closed domain -> a specific subject
sequence to sequence model = LSTM1 + LSTM2





10-11, 12, 13
INTRALOGUE

TASKS/SOLUTIONS:
1. taxonomy of architect topics
2. specific architect chat bot
hashtag to indicates context
dealing with how one feels about a name
named entity classifier
auto-suggest name entity: disney -> disneyland, mickey -> mickey_mouse
consistent context using the most frequent context words
prepossessing of data extracting only noun phrases, getting rid of determinants, verb, prepositions 
train image with words -> suggest images
or just use images already with labels

challenges:
multiple words query
how to find out the categories of words/names/brands/noun ? - wiki categories

use cases: 
envato ceo -> youtube content
hello kitty -> cartoon, arts + other context/topics/catelogues
chelsea, stamford bridge -> soccer

RESEARCH:
stanford nlp papers
end to end memory network + globe
Stanford Named Entity Recognizer (NER)
224n 224d 124
part of speech tagging -> e.g. nnp -> noun, jj -> adj => maybe only extract the noun phrase
named entity recognition
stemming - root stem of the word e.g. writing 's "stem" is writ - porterstemmer e.g. veri import pythonli python
lemmatizing - better -> good
stop words - list of most common words -> e.g. a i he she to etc -> create your own to filter
word net - synonymn using synsets -> look for synonyms/antonyms of words + wordnet.synset("ship.n.01").wup_similarity(wordnet.synset(""boart.n.01"))
scrapy for crawling + apache tika, boiler pipe for filtering main body of text other than side articles
tensorflow - train neural network with label
chunking - grouping parts of speech/regular expression together e.g. group noun + adj, adv into noun phrases -> descriptive phrases surrounding that noun
chinking - chink/remove from chunk e.g. verb, determinant, preposition
stanford named entity recognizer - train your own model http://nlp.stanford.edu/software/crf-faq.shtml
Scikit-learn - multinomialNaiveBayes, gaussianNB, bernoulliNB 



FUTURE PREPARATION:
tensorflow to neural network
amazon to train network
bot to understand question intent + return word to vector result (supervised learning to train label and sentence) BUT HOW DO YOU TELL "i got into harvard" is him trying to be cool -> i dont think its possible given different people have different perspective -> api.ai is just trying to classify "how are you doing" with "what is the things you doing" as same cluster 






WORK LOG:
- try accuracy of cbow model using c
comments: not latest enough, not relevant if too few words

- NLTK library
chunking - regex
see: https://pythonprogramming.net/regular-expressions-regex-tutorial-python-3/
see: https://www.youtube.com/watch?v=imPpT2Qo2sk&index=5&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL
bug in downloading NLTK library is pissing me off -> python3 + nltk.download('all') + central library

- using python to run word to vec

- noisy terms and stop words result using new datasets










10-15 SAT
CNN
filter looks for color contrast and edges
reason using filter -> look for pattern (i.e. edges color contrast tgt) rather than entire picture

what the math behind that filter uses to determine certain pattern occur and in which regions????
weights and biases of convolutional layer affects how operation is performed -> tweaking these numbers affect the effectiveness of filtering process
flashlight structure
the reasons there are layers -> each convolutional layer captures specific type of features 
each layer shares the same weight parameters

relu + pooling -> build up simple pattern discovered by convolutional layer
relu -> recified linear unit
varnishing gradients
relu -> gradients are held constant at each layer during backprop

pooling -> dimentionality reduction -> number of possible patterns becomes exceedingly large -> net focuses on only relevant patterns discovered by convolution and relu



SIRA
Recommendation system: (basic nerual network)
Amazon deep scalable sparse tensor network engine (DSSTINE) -> that works like caffe with only configuration file to set hyperparameters
data-parallel training split data into multiple GPU
model-parallel training split model across multiple GPU
filezilla as gui to aws
convert to netCDF format -> optmized for dsstine -> index files for neurons and features + netCDF file


10-16 SUN
SIRA
Chatbot:(NLP + intent classifier + hard code response OR LSTM, end to end memory network OR API)
IT IS THE YEAR OF CHATBOT

1. Retrieval-based model -> pre-defined responses based on input and context

 Rule-based expression matching with ML classifier
facebook messenger chatbot api -> rule-based expression matching : intent + entities -> hard-code responses to potential questions
-> classifier's main job to understand intent

2. Generative model
does not rely on any sorts of responses -> generate them from scratch

Google research: neural conversation model -> torch + lua 
nerual convo dataset : cornell movie dialog
neuralconvo.Seq2Seq -> 2 LSTM RNN (encoder + decoder)
encoder turns variable input length of sentence into a fixed dimentaionl vector representation -> thought vector 
given enough datasets -> probabilities of thought vectors 
WHY NLL negative log likelihood???
  

LSTM: predicts next sentences given previous sentences in conversation
e.g.
movie dialogue datasets + IT support datasets -> domain knowledge to be expert + conversational knowledge in closed domain 
open domain not possible 


API VERSION:
wit.ai
nuance mix
siri 
api.ai

wit - acquired by fb, fb marketing machine behind them
PROS: speech recognition + open instances(fork other as chatbot backend)
CONS: buggy, sparse docs, no options to create synonyms for entities 

nuance - made by dragon naturally speaking, stick to big enterprise deals  
PROS: GREAT speech recognition
CONS: no instant sign up, not for independent dev

Siri 
PROS: great doc 
CONS: only 6 different app types, only work with apple devices 

api.ai 
PROS: easy to use(simple app in 2 hours) + great docs + integrate/ deploy to service provider of choice + tons libraries 
CONS: charged 

3 parameters to set:

1. domains -> closed domains i.e. context of conversation of source of data e.g. small talk 

2. entites -> a model object refer to in conversation -> CUSTOMIZE SYSNONYMS
i.e. mention of red in convo -> auto detect color 
e.g. flower -> tulips, roses, bulbs (entities synonyms)
e.g. color -> blue, black, red etc  
reference value -> for self reference/self categorization under entity

LIST OF ENTITIES:
lanuage
cities, countries, country code, zip code
unit of measure, temperature
airport, flight number
url, email, address
date, time
color
celebrity, artist, person, organization, first name, given name
age, phone number

 

3. intent (under context) -> customize though vector -> action + sequential intent once intent has been recognized
-> abstraction of user's specific request users made which is then maps to an action (speech response) 
e.g. i'd like to buy some flowers, i want flowers, can i get some flowers -> speech response (sure what kind of flowers you want?)
-> specify context + type specification + output context(system keeps track of what have been said) i.e. set output context of proposal to next 
intent 

CUSTOMIZE SEQ2SEQ MODEL
i.e. proposal intent 
-> what flower intent(flower specification) + looking for flower entity +saveFlowerType + speech response 
-> what color intent(color specification) + looking for flower entity + saveColorType + speech response


 
ml detects statements that are worded differently as the same proposal -> categorizes intent for corresponding responses

if you want to train it yourself using canto -> cluster/classify similar sentences into same intent based on response, thats the only machine learning part of api.ai


SIRA
Read picture generate description:(CNN + LSTM RNN/ CNN-LSTM encoder sentence captions and joint space + mutltimodal space + SC-NLM decoder + GRU RNN skip thought vectors of captions + style shifting + RNN generate front and back stories decode vector to story 

Lasagna walkthrough -> generate.py
Convolu nn ->  pre-trained model/synpses called vgg19
caffe.Net(config.paths['vgg_proto_caffe'], config.paths['vgg_model_caffe'], caffe.TEST)
return an array of features (highest level features right before output layer)
embed image into joint space/ multilingual features model based on unifying visual semntic embeddings
compute nearest neighbours
skip thought vector
gated recurrent units (GRUS) - upper gate and reset gate

skipthoughtsvectors = skipthoughts.encode(z['stv'], sentences, verbose = False)
shift = skipthoughtsvectors.mean(0) - z['bneg'] + z[bpos'] # style shifting
passage = decoder.run_sampler(z['dec'], shift, beam_width=bw)



SIRA
Rap lyrics generator: (unsupervised rhyme scheme identification in hip hop lyrics using hidden markov models)
markov model -> cloudy, sunny, rain
hidden markov model -> calculate rain probability through observing number of umbrellas
rankSVM partially powered by deep neural network -> extract features from rap corpus to feed ot their model
features: rhyming, structural similarity, semantic similarity(requires deep RNN)
rank svm model
Rhyme density


SIRA
self driving car:
SLAM
simulataneous localization and mapping + reinforcement learning
reinforcement learning -> map state, action and reward
end to end neural network

Long term planning for short term prediction -> a paper on new algo to accomplish the task  
Q-learning
BUT HOW IS A SONIC DETECTOR VACCUM CLEANER DIFFERENT THAN MACHINE LEARNING VACCUM CLEANER


10-16, 17, 18, 19
WORK LOG:
1. debug frequent words/bigram bug -> architect names + unique words as bigram
2. nltk -> noun phrases(still need to include more than NN + each article one line), named entity recognizer + stanford NER, lemmatizing, -> translate user input into "topics/categories"
3. crawl architecture library -> gutenberg architect + mit books library + dictionary -> scrapy(web crawler, 10k websites, handle 20-100 url per second) vs nutch(url crawling tons websites) vs pyspider vs selenium vs beautiful soup(parsing webpages)
4. 3 types of data -> articles for word2vec(experimentation, theory, practice) + glossary/theasurus for category filter, bigram + arthur, booktitle with description for Naive Bayes 
5. chunking to be used on extracting user's input
6. rethinking models: 
e.g. Hidden Markov Model (mini-forward algo/forward simulation) -> its not relevant as calculating next word's probability
e.g. LSTM -> word2vec domain specific word vector + short text definition classification

NER
see: http://www.nltk.org/book/ch07.html
convert into IOB format
convert IOB format into NLTK tree

RNN:
intuition of hidden state = the previous word's output state + current word's hidden state
e.g. promisingly utilize water
promisingly -> make the vote it is followed by verb similar to utilize
utilize reinforce the categories of verb (e.g. a positive government verb before resources)

hidden state -> represents a theory (e.g. positive verb, person, negative adj, names, noun)
Wxh -> indicates a word's theory/caetgories
Why -> the presence/sequence of multiple thoeries/categories 's prediction on next word
Whh -> sequence relationship ????


back door: 5123
front door: 5789/8888


config datapath of nltk -> import nltk print(nltk.__file__) # location data.py -> add path to data.py
see : https://www.youtube.com/watch?v=TKAXDqoG2dc&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL&index=9

python pdfminer

regular expression

ReLU (Rectified Linear unit) converges much faster than sigmoid/tanh 6x + does not saturate at least for + region


10-21 typhoon
mini-char-rnn.py
231n assign1

10-22
RNN -> recognize pattern + sequence -> learning syntax rather than semnatics e.g. bogus comment

inside hidden state searching for interpretable cells:
quote detection cell
line length tracking cell -> predicting new line character
if statement cell
quote/comment cell
code depth cell

soft attentio n - ut paper on CNN + RNN
stack up RNN 

10-23 Sun
HackerRank https://www.youtube.com/watch?v=P8Xa2BitN3I
Recursion
Memorization -> e.g. store a list in fibinacci number -> linear run time instead of O(2n)
Dynamic Programming -> counting the path / shortest path
Greedy -> sort first

4 most needed algo:
kd-trees, suffix trees, bin packing problem

see: https://web.cs.ship.edu/~tbriggs/dynamic/
https://courses.csail.mit.edu/6.006/fall11/rec/rec21_knapsack.pdf

knapsack - optimal resources allocation
steven's 1st attempt: recusively try out different combination + store a list of overweighted combination so no need to try again + calculate their weights prioritize in trying
there is a simple brute-force solution -> not exactly sure how to find every possible combination -> NP hard 
there are multiple solutions
Dyamic programmig is one of many ways to improve brute force search
algo intuition - 3 pts

everything could logically be solved by brute force -> all problems are NP until you turn it into P
every problem that has an answer is NP problem until you found an answer

P = NP? -> to reframe the question 
" does being able to quickly recognize correct answers means theres also a quick way to find them?"

polynomial -> N square 7, 2N, N square -> mazes, multiplcation
exponential -> N square N, 2 square N

NP-> bjion computers checking all the solutions at the same time -> then you could find the solution in polynomial time

travelling salesman -> 12-14 vertices if brute force N! time vs O(n^2 * 2^n) 30 vertices 



10-24,25,26 
start from 4:55pm -> phd thesis
cannot sleep -> cal state fulleton 4.0 could not get a decent software engineer position i am so scared 

WORKLOG:
noun + noun phrases
stanford NER
get rid of ' 's and others
plural to single -> lemmatizer
python non-ascii character
strip - convert from utf8 to ascii, strip utf-8 space 
train 4 models -> skipgram + cbow + dup3 -> 4-6 hours training tim
upload code to aws

read hadoop (distributed computing) -> udacity -> mapper -> extract key:value pair from dataset + reducer -> add up values
-> use reg exp to extract number of noun phrases appearance or count word occurence or shopping location total buys or processing logs record how many hits on one page
-> python + matplotlib explore raw dataset -> hadoop streaming with python pydoop transform raw data into feature matrix(i.e. feature extraction using PIG with hadoop as backend) majorly filtering + add up values of relevant variables + hdfs + thread management-> machine learning 
-> airline = load 'airline/delay/$year_str.csv' using PigStorage(',') 
-> airline_flt = filter airline by Cancelled == 0 and Origin == '$airport_code';
->  $data = foreach airline_flt generate DepDelay as delay, Month, DayOfMonth, DayOfWeek,
-> store ORD_2007 into 'airline/fm/ord_2007_1' using PigStorage(',');


231n
batch-normalization -> for random initialization of weight, lower risk of node exploding or dying 
gradients updates -> adam, adagrad, rmsprop, sgd, momentum/sgd
gradients mean how much it magnifies
-> i get chain rule -> chain local gradient to get global gradient -> but i dont get how gradient relates to weight + how to compute gradients (partial derivative)
1 -> since its gradient on its identity function
forward propagation -> stored gradient value 
assuming we want to find out how much changes along each node need to be changed for final result to be changed by 1
e.g. for 1/x, at point 1.37 its rate of change is -1/x^2 -> -1/1.37^2 -> -0.53
i.e. at 1/1.37 -> an increase of 1.37 to 1.38 -> -0.0053 change in f(x), 1.37->1.47 = -0.053 changes
LOCAL GRADIENT DEFINITION: how much a slight increase affects its output values -> -0.53 times
see: https://betterexplained.com/articles/vector-calculus-understanding-the-gradient/
LOCAL GRAIDENT 2nd DEFINITION -> also definition how much locally affect given change by the number on its right


aws -> 
public domain -> security group + set inbound port number: 8880 (flexible, could be other numbers) 
elastic ip is for scaling
elastic beanstalk -> high level load balancing + auto scaling configuration rather than EC2 instance need to set load balancing/ip etc
lambda -> run sh script to install all dependencies -> scale individual function independently -> deal with auto scaling by function and kill the function after running (unlimited running power + 50 mb of code volumns) rather than elstic beanstalk (dealing by opening instances without knowing how many instance to set up) + s3, dynamo db, ses, scs, email, event/trigger support i.e. lambda function will be triggered when user upload file/photo to s3 file storage and dies after running (for function that gets called a lot and we dont have to worry about scaling)
separate instance for front end, backend/database mysql, lambda, nlp machine learning model

chmod 400 -> To protect a file against accidental overwriting 

upload folder -> maybe not every file has the same permission -> zip or github it first

df -T -> check how much physical space left on linux
ls -lh ubuntu

the reason they put a name on the tech is because your brain automatically stop asking how the tech works when there is a name -> abstraction

java path /usr/lib/jvm/java-1.7.0-openjdk-amd64/bin



10-27,28
WORKLOG
setting up environment for ubuntu
stanford ner bug
nltk
shell script - use python to run shell script, use shell script to run python
running model 
filtering of noun phrases less than 4 digits
filtering with manual vocab + frequency + first and last paragraph
for only noun -> set ism or other similar rules to determine whether a noun should be added to word2vec -> too many generic words in word2vec -> or just get rid of some likr 4-5 words


IDEA: use word2vec to improve results of websites and search engines

shell script - 
cat /etc/passwd | sed
cat command dumps the contents of /etc/passwd to sed through the pipe into sed's pattern space 


10-29
hallowen
I love meeting a truly nice girl.
I love the feeling of becoming an awesome person because she is awesome.

10-30
Backpropagate -> calculate local gradient
Local gradient -> means rate of change of Loss with respect to that neuron
Local graident -> +/- loss taking the next slight step
one time step -> before weight updates/one epoch
training -> forward(loss), backward(gradient), update(weight using gradient as past mistake taking a slight step) -> forward backward update
Jacobian matrix -> derivatives of each element of z w.r.t. each element of x -> when doing backprop, only care about gradient vector
Vectorized operations -> minibatch, process 100 examples at one time
Linear classifer -> i.e. no hidden layer -> can capture existence of features of a car -> BUT CANNOT deal with different colors/car facing different direction -> hidden layer helps wiggle -> i.e. one hidden layer just calculating red car facing forward -> given relu max(0,x) function only found feature will have positive hidden node -> theory/car templates/mode -> in practice likely to be blended rather then clean cut theory 


1 million dollar question : WHY using -learning rate * gradient to do updates -> coz technically speaking gradient just indicate magnitude of total error change when w changes
-> BECAUSE it only indicates the mistake/loss caused by the past step/rate of change at that moment i.e. you are trying to take a step back thats why learning rate is negative -> think about the gradient descent 3D graph where you do not know where to go -> you are slightly adjusting one step to try to find the path -> UPDATE really just means given the indication of mistake/loss past step will cause an increase in overall loss, we pick a learning rate/step size

WARNING: There might not be negative loss depending on the loss function:
NOTE: gradient means +/- loss taking a step forward
Neg loss + Pos gradient -> taking a step back will cause positive increase/lower loss
Neg loss + Neg gradient -> taking a step forward maybe cause less neg loss/lower loss
Pos loss + Pos graident -> since taking a step forward will result in pos loss -> take a step back -> maybe result in negative decress/lower loss 
Pos loss + Neg gradient -> since taking a step forward will result in neg loss -> take a step forward -> reult in negative loss 




what is hell is cuda code

10-31 

WORLOG:
check file size
see: http://stackoverflow.com/questions/20031604/amazon-ec2-disk-full
sudo du -a / | sort -n -r | head -n 10
df -h
ls -lh


Latent Dirichlet Allocation Topic model
-> tells us what topics are present in any given document by observing all words in it -> and producing a topic distribution

only observable variables are the words
latent variables -> topic, inferred and assigned to each word in the document 
topic distribution for document i

take the recipe and generate a document based on model's rules

alpha -> per-document topic distribution -> high alpha -> likely to contain mixture of topics -> make documents more similar to each other
beta -> per topic word distributions -> high beta -> for each topic likely to contain most of the words -> make topics more similar to each other

SVD vs generative model approach 

posterior inference to uncover latent variable

dirichlet distribution is just a disribution over multinomial distribution

document -> probability distribution over topics
topic -> probability distribution over words

cluster discrete data 

think k means but not k means

return -> k number of topic clusters with its related words + for each document which likely topic 

conjugacy -> this posterior has same form as prior
draw topic from dirk distribution with some prior
for every document draw distribution over topics from dirk distribution
once we have distribution of topics -> inference i.e. chose topics to represent words

latent variables:
z -> topic for each word in document -> topic assignments
beta -> for each topic, words representing the topic -> topic itself
theta -> each document distribution over topics

to figure out latent variables: 
gibbs sampling
-> markov chain monte carlo

gamma function identity

so the learning is learning the higest probability of both probability

until the likelihood converges/it gets tired where it wont change anymore

basically random guess

statistical inference -> filling latent variables
model precision vs held out likelihood 

collasped gibbs sampling 

E-discovery in court cases 

latent semantic analysis

> single angle bracket  ->. overwrite
>> double angle bracket  ->. append

11-1, 2, 3, 4

WORKLOG
LDA -python
shell script to train model
eliminate words short than 5 + architecture word + not combining 3 or more noun phrases
LDA2vec
t-SNE dimentionality reduction
chainer deep learning library
gaussian processes
tensor decomposition
tf-idf term frequency inverse document frequency


same vector space -> means same dimentionality vector


Microsoft Research Paper
1. BLC - Basic level categorization
-> Probase, KnowItAll, NELL -> data driven knowledge base 
-> is-a relations (inheritance vs has-a composition)  matching Hearst patterns e.g. president such as Obama -> extracts Obama as an instance of concept President 
-> typicality i.e. how typical is an instance for a concept -> e.g. P(president|obama) >
P(author|obama) -> is-a/hearst pattern + typicality + scring/ranking -> not sufficient for BLC task as P(c|Microsoft) -> company which is too generic P(Microsoft|c) -> largest OS vendor too specific/limited coverage
-> typicality with smoothing
-> PMI
-> Implicit knowledge mining -> LSA, PLSI, LDA -> computation heavy, not on large scale data, not easily interpreted by humna -> OTHER EFFORTS -> get the term embedding by using deep learning techniques, this embedding could be treated as represtnation for a given term in high-dimensional semntic space, but usually embedding is not good for low frequency terms ???????

2. ProBase



PAPER TOPIC
improvement on LDA using BLC, typicality, PMI 



sigmoid is still used in LSTM for its special reason 

Batch training vs epochs:
For batch training all of the training samples pass through the learning algorithm simultaneously in one epoch before weights are updated.
https://www.mathworks.com/matlabcentral/answers/62668-what-is-epoch-in-neural-network

activation function is judged by how the gradients flow back into the network e.g. dead relu

see: http://stackoverflow.com/questions/26745519/converting-dictionary-to-json-in-python
python dictionary with list -> json format
json.dumps() converts a dictionary to str object,not a json(dict) object!so you have to load your str into a dict to use it by using json.loads() method!

sparse matrices
see: https://www.youtube.com/watch?v=Lhef_jxzqCg
CSR -> compressed sparse row
-> creates compressed sparse row from a matrix containing 0 with 1. value in a sequence (values)  2. columns of the value (column indices) 3. start of the row in terms of elements in value sequence (row offsets)


CNN
activation map -> convolve/slide over all spatial locations -> activation of this fiter at every spatial location of the input volume
entire filter bank -> e.g. 6 independent filters produce 6 activation maps -> covolutional operation

FILTER INTUITION
filter -> looking for a theory or a combination of theories e.g. straight line, curve line, color
6 layers of filters -> looking for 6 independent sets of theories
activation map -> thoery result
6 layers of activation map -> each layer represent one set of theory result
filters on activation map -> a more sophicated theory/combination of theory on existing/found theory result -> THEREFORE passing through layers activation map indicates sophicated features
next convo layer putting together features of previous convo layer

MAXPOOL INTUITION

AVGPOOL

CONCAT
edge detection 
shape detection 
increasingly more abstract up till the end


SIRA
Movie Recommendation
see: http://www.hongliangjie.com/2012/08/24/weighted-approximately-ranked-pairwise-loss-warp/
WARP: weighted approximately ranked pairwise loss -> loss function 
LightFM -> python wrapper over c 

SIRA
Tensorflow
Image classifier 
inception by Google
transfer learning -> apply knowledge of previous training session to new training session (inception model not trained on dark vardar but apply on dark vardar) -> retrain features of dark vardar on last layers to add representation of dark vardar to repository knowledge of inception model

install Docker
install TF Image


11-6,7 Sat, Sun
RUN TENSORFLOW !!!!!!!!
WRITE CUDA CODE !!!!!!!!
RUN TFIDF !!!!!!!!
RUN IDA2VEC !!!!!
-> word2vec vector [0.75, -1.25, -0.55, -0.12, +2.2] -> all real values -> address, 200 main street is not twice 100 main street -> similar in 100d ways 
-> LDA document vector [0%, 9%, 78%, 11%] -> all sum to 100%
P(v out | v in + v doc) where v doc comes from LDA, v in comes from word2vec
combine input pivot word with document vector -> document vector capture long distance dependencies where word vector captures the short distance dependencies
doc2vec/paragraph vectors -> downside doc2vec isnt as interpretable as LDA topic vectors -> missing mixture and sparsity
TURN v doc INTO A MIXTURE
doc vector is decomposed into several word2vec vector -> i.e. a vtopic1 + b vtopic2 + ... where a or b represents probability of that topic
using doc2vec to decompose a document vector into word2vector that is word representing the document topic
-> might not be applicable since return p(v out | v in, v topic, v docid, v zipcode, v clientid) using doc2vec
document vector is linear combination of topic vectors
topic vectors live in the same space as word vectors
paragraph vector to doc2vec
-> word vector model 
-> prevent co-adaption -> perfrom dropout

BUILD RNN !!!!!
RUN LDA2LSTM LDA2AE !!!!!!! 
-> give me a sentence 80% religion, 10% flirty


Dirichlet distribution
-> probability simplex, probability mass function sums to 1
-> density function
-> distribution over n-d vectors
-> the reason its triangle is because we are just picturing a 3d world

bag-of-n-grams
textblob




11-8,9,10,11

Negative sampling 
see: http://stackoverflow.com/questions/27860652/word2vec-negative-sampling-in-layman-term
-> The idea of word2vec is to maximise the similarity (dot product) between the vectors for words which appear close together (in the context of each other)
-> and minimise the similarity of words that do not

Parameterizing skip-gram and modeling the conditional probability p(c|w; θ) using soft-max
      v_c * v_w
 -------------------
   sum(v_c1 * v_w)
-> numerator is basically the similarity between words c (the context) and w (the target) word 
-> denominator computes the similarity of all other contexts c1 and the target word w
-> Maximising this ratio ensures words that appear closer together in text have more similar vectors than words that do not
-> However, computing this can be very slow, because there are many contexts c1. 
-> Negative sampling is one of the ways of addressing this problem- just select a couple of contexts c1 at random
-> e.g. if cat appears in the context of food, then the vector of food is more similar to the vector of cat (as measures by their dot product) than the vectors of several other randomly chosen words (e.g. democracy, greed, Freddy), instead of all other words in language

Doc2vec, aka paragraph2vec, aka sentence embeddings
Distributed Memory Model of Paragraph Vector (PV-DM)


LDA revisit
dirichlet distribution is distribution over multinormial distribution i.e. probabiilty of multinormial distribution
multi. distribution comes from dirichlet with a and m as parameters deciding what the distrubtion is going ot be
a -> scale 
m -> centre
if distribution is far away from the centre -> prefer one or more of the corners and strongly disprefer at least one fo teh corners i.e. only one to two labels, food but not techlogy or flowers -> like how human only think of 2-3 topics when writiing documents

latent semantic analysis -> use SVD/ singular variable decomposition to decompose number of documents + vocabs -> doc topics * topcis vocab matrice multiplication 

Difference: LSA make no assumption about distribution of topics -> LSA documents contains entry of every topic in topic assignment

smaller alpha -> more selective in things draw from base distribution

we draw our topics from dirichlet distribution so we have control over our topic distribution

for every document we draw distribution over topics from dirichlet

generative model -> assigning topics to word / generative words fro those topics
for each topic -> multinomial distribution over words
each document -> multinomail distriubtion over topics
using statistical inference to figure out the latent variables

gibbs sampling equation aka markov chain monte carlo -> for loop each word, take off its topics, compute combined probability from document and total words given prior distribution -> update new topic

alpha, lambda -> prior distribution -> slicing sampling them too

number of chains -> how many times you run this algo -> should run more than once -> passes?

lag/burn-in -> final answer or average everything you seen in past answers -> e.g. take mini chains, take mini examples from mini chains or one chain with large lag and sample every 10 thousands iteration 


cs276
Merge algorithm proximity queries positional index
-> biword indexes -> index every consecutive pair of terms inthe text as a phrase -> turn each of bi-words into dictionary term
-> <term> : <doc>
-> inverted index -> for each term t -> store a list of all documents that contain t + identify each doc by a docID, a document serial number
-> data structure for inverted index -> x fixed size array -> variable size postings lists e.g. linked list/variable length arrays to store the documents in which the word occur

Ranked retrieval models
-> rather than satisfying query expression -> ranked retrieval retunrs ordering over documents in the collection
-> Query document matching scores to a query/document pair 
e.g. probability / frequency
e.g. jaccard coeffiicient -> does not consider term frequency -> only uses set + rare term in collection more informative than frequent term
tf-idf frequency weighting

Document frequency -> number of documents that contain t -> inverse measure of informativeness of t
idf = log(N/df) -> num documents/occurence of that word in document -> bigger means rare term i.e. idf = how rare the term is -> log dampen, less the idf
e.g. 1 million document -> calpurnia(appear only in one ducment) idf = 6, animal (100 documents) idf = 4, the(appear in all documents) idf = 0
idf -> small multifier to pay more attention to rare terms
effect of idf on ranking -> if only 1 term query e.g. iphone all documents all share same scaling factor -> whereas if mulitple worded query -> pay more attention to documents comprising capricious than documents comprising person when the query is capricious person
Collection frequency -> number of occurences counting multiple occurences i.e. total number appearance -> but Document frequency is prefered as that indicates rare words e.g. insurance appears more in same document but in less documents even it has the same collection frequency as insurance -> insurance is still a rarer word/more informative than try


Term-document count matrices
-> bow model -> bow vector representation does not consider the ordering of words in document -> BUT we only consider the number of times word occur in a document
-> positional index was able to distinguish ordering whereas bow cannot -> positional index is able with proximity or phrase queries

Term frequency weighting to compute retrievel score
-> relevent but not linearly -> come up with some way of scaling term frequency -> log-frequency weighting 

Query processing + permuterm index + wild card queries + bigram/kgram 

Term-frequency Inverse document frequency
-> w = (1+log tf) * log (N/df) 
-> best know weighting scheme in information retrieval
-> increases with the number of occurences within a document + increases with raity of term in the colleciton
-> e.g. calpurnia person -> documents with higher terms of calpurnia will rank higher
-> turn ranking into a weight matrix
-> weight matrix indicates how much that words suggests that document 
-> each document is represented by a real-value vector of tf-idf weights
-> tf.idf -> term document matrix 


Query as vector
-> represents query as vector in the same space
-> rank document according to proxmity of query in the space
-> proxmity = similarity of vectors -> i.e. inverse of distance -> rather than boolean value return -> how well the score -> rank relevant document higher

Formalizing vector space proximity 
-> euclidean distance i.e. direct distance on vector space / absolute difference between end points of vector  
-> cosine looks at angle -> angle matters as if 2 vectors pointing at same direction then they are similar -> intuition duplicate document's by its content = multiply word vector's magnitude by 2 times -> angle between 2 word vectors 0 -> maximal similarity
-> COSINE SIMILARITY = RANK DOCUMENTS ACCORDING TO ANGLE WITH QUERY
-> cosine is used because between 0-180 degrees it is decreasing i.e. bigger the angle more negative the value 
  
Length normalization 
-> length of a vector is just distance of vector end points to zero
-> a vector can be length-normalized by dividing each element of the vector by its length (l norm) i.e. 3/5, 4/5 
-> dividing a vector by l norm/length makes it a unit vector i.e. 3/5^2 + 4/5^2 = 1
-> intuition of longer vector with same angle is similar with shorter vector with same angle e.g. duplicate same words in the document/query

Cosine similarity
-> intuition: after normalize it is measuring the probabiliity of that aspect -> if both have high probability of that aspect then high similairyt

Sam redmond
PVDM -> memory model of paragraph vectors i.e. doc2vec -> to group similar classes tgt 
Naive bayes (so i guess naive bayes does not only look at computer programming but also other similar words for results)


11-12, 13 sat sun
cs224d assign1 - done q1, stuck at q2 word2vec

SIRA
chat bot - google neurla conversation model
see: https://arxiv.org/abs/1506.05869
https://www.youtube.com/watch?v=5_SAroSvC0E

Torch + lua
movie + it desk support data sets

sequence learning model -> learn data with long range memory dependency
lstm turns input vector of variable length into fixed vector representation -> aka thought vector
training the closeness between questions 
negative log likelihood -> log probabaility of input data -> improve sentence prediction
learning rate and momentum -> pace time step
decay factor + minMeanError -> improve learning rate


11-14, 15, 16, 17, 18 
WORKLOG
1. compiling dictionary -> problem not enough words in dictionary -> stanford ner + tree for efficient filtering
2. slda paper
3. tf-idf information retrieval 
4. probase -> not appplicable since architect named entity too specific -> regular expression


tf-idf as ranking algorithmi
best known weighting scheme in information retrieval
free text queries i.e. no operators or expressions -> we need ranked retrieval models
assign a score to each document -> score indicates how well a query and document pair 
-> term document incidence matrix i.e. bow model
-> term frequency is relevant but not linearly -> 1 + log frequency if tf > 0
-> score -> sum of all matching terms log frequencies
-> inverse document frequency idf = log(N/df) where N: number of documents, df document frequency of t i.e. number of documents that contain t
-> idf, the bigger the document + smaller the document that contains the term -> rare score
-> a word that occurs in every document has a zero idf score and thus 0 tf.idf score
-> collection frequency vs document frequency -> df is preferred since rareness of a term should not be affected by the sheer fact it appears many times in one document

jaccard coefficient-> query-dcoument match score
-> overlap between 2 sets A n B
-> i.e. match / total number of words
-> all else being equal shorter one is prefered
-> X consider term frequency
-> right now normalization using total terms rather than consine simialrity i.e. vector similarity

normalize 
-> turn document vector into a unit vector 
-> so document ranking will not be affected by the number of words it contains-> a document with more words has higher real value word vector 

overall
-> tf.idf for each rare word -> we look into the term frequency of that word inside the document

vector space proximity model
-> document -> real value vector
-> represent query as vector in the space + rank documents according to proximity to the query in the space -> proxmity = similarity fo vectors = inverse of distance -> cosine similarity, X euclidean distance (given the same word -> same rariness -> longer vector means a higher term frequency of same mixture of words)
-> rank doc in decreasing order of angle between query and document + rank document in increasing order of cosine
-> comparing td.idf weight of CO-EXISTING TERMS in query and document (or you could compare cosine similarity of document vector)

SMART NOTATION
->allows different weighting scheme algorithm in making queries
-> standard weighting scheme -> lnc.ltc (i.e. document vector no idf + query side tf.idf)
-> lnc.ltc -> compute only query side idf + document side tf -> more efficient + rareness already taken in consideration
-> dot product -> sum dot product up -> that is the cosine simialrity
-> since unit vector pretty simialr value at high cosine similarity region -> ranking more important than absolute value difference 

COSINE SIMILARITY
intuition of cosine similarity -> 1. draw an 90 degree angle between 2 unit vectors 2. |A| cos(@) |B| = A.B where |A|cos(@) is the converted distance of A on B 3.|A|cos(@)|B| bigger the angle smaller the cosine hence smaller dot value between vector given same unit size

COSINE SCORE IN VECTOR RETRIEVAL SYSTEM

HOW I MET YOU MOTHER + TFIDF
see: http://www.markhneedham.com/blog/2015/02/15/pythonscikit-learn-calculating-tfidf-on-how-i-met-your-mother-transcripts/
see: http://www.markhneedham.com/blog/2016/07/27/scitkit-learn-tfidf-and-cosine-similarity-for-computer-science-papers/
see: https://janav.wordpress.com/2013/10/27/tf-idf-and-cosine-similarity/
see: http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/
see: https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html
see: http://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity

scikit-learn 2 ways to generate tf.idf representation
1. CountVectorizer(matrix of token counts) -> TfidfTransformer(tf.idf representation)
2. TfidfVectorizer (straight to tf.idf representation)


11-19 sat, sun
224d LSTM
see: http://colah.github.io/posts/2015-08-Understanding-LSTMs/

1. forget gate layer ->  decide what information to throw away from cell state -> intuition e.g. france, girl and other features of present subject -> when we see new subject, forget the gender of old subject

2. input gate * new candidate values
what information to store in cell state
first, sigmoid layer called "input gate layer" decides which value to update
then, tanh layer creates a vector of new candidate values that could be added to the state
then, combine "input gate" + "new candidate values"

3. do the update to cell state
multiply old state by f 
add new state to replace old state we are forgetting -> i*C -> new candidate values scaled by input gate

4. output as hidden layer
filtered version of cell state -> sigmoid layer to decide what parts of current thoery to output to current hidden layer
put the cell state through tanh ->  multiply filtered version of current thoery -> we only output parts we decided to

LSTM
f = sigmoid(W * [h_prev, x]) -> forget/ C_prev gate
i = sigmoid(W * [h_prev, x]) -> input/ C_candid gate
C_candid = tanh(W * [h_prev, x])
C = f * C_prev + i * C_candid
o = sigmoid(W * [h_prev, x])
h = o * tanh(C)

just focus on LSTM Cell state, forget the gate for the time being

GRU - simplified version of LSTM -> substitute Cell state with h_candid + just think tanh + think sigmoid separately + tanh is used for actual prediction + sigmoid is only gate
z = sigmoid(W * [h_prev, x]) -> h_candid keep gate i.e. how much h_prev affects h
r = sigmoid(W * [h_prev, x]) -> h_prev keep gate i.e. how much h_prev affects h_candid
h_candid = tanh(W * [r * h_prev , x])
h = (1-z) * h_prev + z * h_candid

difference:
LSTM tanh(h_prev) -> C_candid -> f*C_prev + (1-f)*C_candid -> o * tanh(C)  VS   GRU tanh(r * h_prev) -> h_candid -> (1-z)*h_prev + z*h_candid
LSTM one more step to let current output decide while GRU lessen the effect of h_prev from the get go

Seq2Seq model LSTM with attention mechanism
acutal paper: http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf

seq2seq model introduced: http://arxiv.org/pdf/1406.1078.pdf
multilayer cells in seq2seq: https://arxiv.org/pdf/1409.0473.pdf
attention mechanism to allow decoder peek into input at every decoding step: https://arxiv.org/pdf/1409.0473.pdf
see: https://research.googleblog.com/2015/11/computer-respond-to-this-email.html

A multi-layer sequence-to-sequence network with LSTM cells and attention mechanism in the decoder 

one used to encode incoming email
one used to predict possible responses
encoding network consumes words of incoming email -> produces a thought vector
thought vector -> captures gist of what is being said without getting hung up on diction e.g. are you free tmr thought vector -> similar to does tmr work for you
second network/decoder starts from this thought vector -> synthesizes a grammatically correct reply
encoder -> process input into thought vector -> maps variable length source sequence to fixed length vector 
decoder -> use thought vector to generate output -> decoder maps vector representation back to variable length target sequence 
scoring phrase pairs using phrase-based SMT system with RNN Encoder-decoder 
decoder -> generate output sequence by predicting output symbol given hidden state ->  y_current_timestep and h_current_timestep conditioned on y_pre and c_summary_of_input_sequence

MULTIPLE LAYER
multilayer -> use thought vector + EOS token as hidden layer -> LSTM train output vector with weighting on thought vector
see: http://stackoverflow.com/questions/38248185/multi-layer-lstm-with-more-than-one-memory-cells
"multi-layer" means stacking the LSTM units; -> looking for complex pattern with feature representation from hidden layer from lower level
"memory cells" means hidden units.

see: http://stats.stackexchange.com/questions/163304/advantages-of-using-multiple-lstm-s-in-deep-network
see: https://www.quora.com/What-is-the-difference-between-stacked-LSTMs-and-multidimensional-LSTMs
Stacking -> In case of a simple feedforward net we stack layers to create a hierarchical feature representation of the input data
higher LSTM layers can capture abstract concepts in the sequences

FEED-FORWARD STACKING VS LSTM STACKING
feed forward layer (say a fully connected layer) does not receive feedback from its previous time step and thus can not account for certain patterns. Having an LSTM in stead (e.g. using a stacked LSTM representation) more complex input patterns can be described at every layer

MULTI-DIMENTIONAL LSTM VS STACKED LSTM
see: https://www.quora.com/What-is-the-difference-between-stacked-LSTMs-and-multidimensional-LSTMs
multidimensional LSTM units have recurrent connections in multiple dimensions.


INTUTITION
USING TREE AND DISTIRBUTION TO RETHINK NEURAL NETWORK


LSTM -> generate next word -> what about generate a meaningful response

Multinomial distribution 
- a table of ocurrence -> a table when normalized become probability

11-21, 22, 23, 24

WORKLOG
BM25 + 2 poisson - Harter classical model
- equation looks like tf.idf
- BM25 dervies from binary independent model
- tf.idf always assume more is better -> e.g. "blog: my holiday in Beijing" "Economic development of Sichuan 1920-1930"
- coord factor reward document when all terms matched
- tf.idf is a heuristic that makes sense intuitively but somewhat a guess
- root of BM25 -> probabiliity ranking principle -> "order retrieved documents by decreasing probabililty of relevance"
- Relevancy in ranking
- binary independence model -> R: relevant documents r: relevant documents containing query term n: documents containing query term N: all documents -> dont care frequency at all

P(F=1|R=1) = r+0.5/R+1
P(F=1|R=0) = n-r+0.5/N-R+1

Robertson/Sparck Jones-weight


EXTARCTING KEYPHRASES + TOPIC MODELING
TEXTRANK -> DIVRANK -> CONUNDRUMS IN UNSUPERVISED KEYPHRASE EXTRACTION, SINGLERANK, EXPANDRANK, COLLABRANK, PAIRWISE RANKING SVM
see: http://www.slideshare.net/andrewkoo/textrank-algorithm
see: http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/
The canonical unsupervised approach to automatic keyphrase extraction uses a graph-based ranking 
Matrix to solve + all unknown sum to 100% i.e. normalize + divided voting system

PageRank- MATRIX FORMULATION
flow equation could be rewritten as r = M * r where flow equation equals to computing and normalize voting of each node

stochastic adjacency matrix vs column stochastic matrix i.e. columns sum to 1 

SIMILAR MATRICE
similar matrices A and B -> M.T.dot(A.dot(M)) where M is a matrix not A or B
all similar matrices have same eigenvalues



DIVRANK 
1. markov chain
see: https://www.youtube.com/watch?v=uvYTGEZQTEs
transition diagram 
transition probability matrix


PAIRWISE RANKING SVM
see: https://www.youtube.com/watch?v=2UpLin5T_E4
see: http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/
see: https://www.youtube.com/watch?v=dKppAG0cdkM
query + click -> SVM to training document ranking (click means relevancy +1, -1, query -> docuemnt query feature representation) with pair wise
pair wise generated from click after query generates SVM constraints
leanring with click data - extract features of documents and train wiht user's click -> label : relevant / not relevant
document query feature representation -> pair wise transform i.e. picking features that emcompasses both query and document e.g. cosine similarity 
featrure representing (Doc, Query) pair -> assign weight -> SVM with label +1, -1 (from click)

SIMILARITY OF DOCUMENTS




11-25, 26, 27 thanksgiving, sat, sun

HIGH DIMENTIONAL VISUALIZATION
put all the x1,x2,x3 ... xn on same axis with other axis as y(value) and increases or decrease all x1,x2...xn at the same time


LINEAR ALGEBRA
looks like linear algebra is finale of high school math - combining all math in linear algebra space


- SINGULAR VALUE DECOMPOSITION

PRINCIPAL COMPONENT ANALYSIS
A = U * S * V.T
A.T * A = 

U -> ORTHOGONAL MATRIX (rotate) -> right singular vectors
S -> DIAGONAL MATRIX (stretch) -> sigma instead of eigenvalues -> singular values
V.T -> ORTHOGONAL MATRIX (rotate) -> right singular vectors

V * LAMBDA * V.I

see: https://www.youtube.com/watch?v=cOUTpqlX-Xs
C.T * C = V * S.T * S * V.T (diagonalization of C.T * C i.e. eigenvalues)
determinant(C.T * C - LAMBDA * I) -> decompose into quadratic equation 

C.T - lambda * I = V 
column of V should be unit vector because it is orthonormal


- ORTHONORMAL 
unit vector 

- NORM SPACE OF VECTOR 
- NULLSPACE
- DETERMINANT
- LAMBDA
- IDENTITY MATRIX

- SUBSPACE

S.P (s perp)

- ORTHOGONALITY
Orthogonal vectors & subspaces

both vectors and subspaces and matrices could be orthogonal
angle between rowspace and nullspace 90
angle between column space and nullspace 90 

vectors are orthogonal to each other if X.T * Y = 0
pythagoras
X.T * Y = 0 -> dervied form a^2 + b^2 = c^2
based on 2 properties co-occur -> if A.T * B = 0 -> A.T * A + B.T * B = (A+B).T * (A+B)
dot product of orthogonal vector -> 0 

subspace orthogonal to subspace
-> every vector in every vector in S is orthogonal to every vector in subspace T

nullspace perpendicular to rowspace

columns that are orthonormal sets 

components, projection of vectors

- LINEARLY INDEPENDENT


- RECTANGULAR MATRIX

- EIGENVECTOR, EIGENVALUE
see: https://www.youtube.com/watch?v=DzqE7tj7eIM 
solve system of linear equations
system -> more than 1 equations

eigenvalue-> denoted LAMBDA
eigenvalues -> solve differentiation equation
transform matrices to value
but where does this eigenvector comes from 
2x2 matrix -> 2 eigenvalues / eigenvectors
if eigenvector stays the same -> eigen value is squared

A = B * x -> A * x = lambda * x

1. A - (I * lambda)
2. determinant -> solve for lambda
3. using determinant answer to solve A - (I * lambda) -> eigenvalue 
4. plug new eigenvalue in -> original matrix A -> new matrix B
5. solve B * x = 0 -> x eigenvector  


11-28, 29, 30, 1, 2
WORKLOG

cs124

LANGUAGE MODELING -> syntactic/gramatic probability of a sentence -> chain rule of probability 
-> probability of a sequence of sentence -> product of P(w|w,w,w,w,w)
apply simplified assumption called Markov Assumption -> i.e. compute 1-3 previous words i.e prefix of last few words

MARKOV ASSUMPTION -> n-gram -> screw up everything and neural network come to rescue
unigram -> most frequent word -> random sequence of words based on term frequency
bigram -> slightly more intelligent, conditioned on single previous word -> but no connection over long sequence of words
n-gram model in general -> insufficient, long distance dependencies i.e. computer, which … is crashed -> crashed is related to computer

RELATIONS EXTRACTION
structured knowledge base

TEXT SUMMARIZATION
textrank

AHO-CORASICK AUTOMATA
substring/pattern string in text string T

suffix link
- look for substring that contains words already looked up -> look up remaining words in text
- When we hit a part of the string where we cannot continue to read characters, we fall back by following suffix links to try to preserve as much context/looked up words as possible
- every node in the trie will have a suffix link associated with it 
- algorithms whose runtime depends on how much output is generated referred to as output-sensitive algorithms

TRIES SUFFIX TRIES
parallel searching -> trie
see: http://web.stanford.edu/class/cs166/lectures/02/Small02.pdf
see: https://www.youtube.com/watch?v=hLsrPsFHPcQ
see: http://www.geeksforgeeks.org/trie-insert-and-search/
Trie is an efficient information retrieval data structure. Using trie, search complexities can be brought to optimal limit (key length). 
Every node of trie consists of multiple branches. Each branch represents a possible character of keys. We need to mark the last node of every key as leaf node. A trie node field value will be used to distinguish the node as leaf node (there are other uses of the value field)
digital tree
radix tree
prefix tree

Suffix trie - check whether contains substring
see: https://www.youtube.com/watch?v=hLsrPsFHPcQ
-> not itself very effective data structure, but good introduction to other suffix data structure
-> build a trie containing all suffixes of a text T -> check whether string S is a substring of string T -> prefix of suffix of T -> fall off trie i.e. no outgoing edges -> S not a substring of T -> exhaust without falling off the trie -> yes substring
-> first add terminal character $
-> $ indicates end of a suffix when encode in trie
-> $ enforce the rule to put words in dictionary order, shorter of the word goes first 
-> encode all possibility of suffix into a trie -> each path from root to leaf represents a suffix + each suffix is represented by some path from root to leaf
-> number of times occur -> number of leaf nodes below path given we don’t fall off -> leaf node can be counted via depth-first traversal

Suffix Tree - find all substrings
compressed suffix trie
binary tree -> a tree where all non-leaf nodes have exactly 2 children
suffix tree -> a tree where non-leaf nodes could have more than 2 children




RANGE MINIMUM QUERY
- minimum number in an array

LOCK 951

RELATION EXTRACTION - structured knowledge base
see: https://spark-public.s3.amazonaws.com/cs124/slides/rel.pdf


Relations triples
- e.g. Stanford LOC-IN California
- extract entities relations!!!

Current knowledge base 
-> WordNet, Freebase, DBPedia -> support question answering
-> adding new structured knowledge to existing current database


Automated Content Extraction (ACE)
-> 17 relations grouped into 6 classes e.g. family, business, founder, ownership, sports affiliation, alum, user-owner-inventor-manufacturer, subsidiary
-> e.g. Donald Trump is in Tennessee -> physical-located (class, relations) relations
-> e.g. relation kind -> physical-location, ORG-subsidiary, person social-family, ORG-Founder

Unified Medical Language System: UMLS
-> customize entities and relations for different tasks

Resource Description Framework (RDF) triples
- subject predicate object e.g. Golden-Gate-Park location San-Francisco

DBPedia 
- draws on wiki info boxes to create1 billion RDF triples

Freebase 
- frequent freebase relations -> person/person/nationality, person/person/profession, people/person/place of birth

Ontological relations
- IS-A hypernym -> subsumption between classes -> e.g. Giraffe IS-A ruminant IS-A ungulate IS-A mammal IS-A vertebrate
- instance-of -> e.g. SF instance-of city



HOW TO BUILD RELATION EXTRACTORS
- hand written rules
- supervised machine learning
- semi-supervised, unsupervised -> bootstrapping using seeds, distant supervision, unsupervised learning from web


1. Hand written rules
- Hearst (1992) for extracting IS-A relations i.e. red algae such as gelidivum
- too many patterns, problems reusing transferring to other domains

Richer relations
- relations that hold between specific entities e.g. located-in, founded, cures
- e.g. George Marshall, Secretary of State of United States -> 3 named entities, 1 pattern “PER, POSITION of ORG”


2. Supervised machine learning


-> KEY: extract entities-based features 
-> BUT YOU NEED TO HANDLABEL RELATIONS BETWEEN ENETITIES + DO NOT GENERALIZE WELL TO DIFFERENT GENRES

- 2 classifiers -> 1. find all named entities in same sentence 2. decide if entities are related 3. if yes classify their relations
- 2 classifier -> faster classification by eliminating most parts + use distinct feature sets appropriate for each task e.g. ACE

Examples of features 
see: https://www.youtube.com/watch?v=Mgz2Ma2NzuM
- word-based features 
- syntactic features
- e.g. headwords Airlines of American Airlines -> bag of words, bigrams
- Gazetteer and trigger word features e.g. trigger list for family kinship terms: parent wife husband grandparent or geographical location places
- Gazetteer -> subentities relations -> sanfran/california/us

Evaluation
precision recall-> true gold relations -> take the balance


3. Semi-supervised + Unsupervised machine learning

Seed based, Bootstrapping
- a few seed tuples and high precision patterns
- bootstrapping -> use seeds to directly learn to populate a relation

Relation bootstrapping (hearst 1992)
- gather seed pairs that have relation R
e.g. Seed tuple <Mark Twain, Elmira> 
-> google to grep all patterns of that seed tuple -> use that pattern to grep more other seed tuple -> for loop

Dire: extract <author, book> pairs - segie brin 1998
-> start with 5 author book seed tuples

Snowball 2000


TEXTRANK
see: http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/
implemented in python

DivRank
ExpandRank
CollabRank
Annotated Suffix Tree Similarity

improvement on textrank
1. normalization of candidates -> i.e. normalize term frequency -> taking average of incoming score 
2. co-occuring windows size, weighted graph

Pairwise Ranking SVM
-> given a query, rank based on click
 

LAMBDA
see: http://stackoverflow.com/questions/890128/why-are-python-lambdas-useful
see: http://www.python-course.eu/lambda.php
Python supports a style of programming called functional programming where you can pass functions to other functions to do stuff
lambda operator or lambda function is a way to create small anonymous functions, i.e. functions without a name
These functions are throw-away functions, i.e. they are just needed where they have been created

Dropwhile()
Takewhile()
see: https://pymotw.com/2/itertools/

Dropwhile() function returns an iterator that returns elements of the input iterator after a condition becomes false for the first time. It does not filter every item of the input; after the condition is false the first time, all of the remaining items in the input are returned.

The opposite of dropwhile(), takewhile() returns an iterator that returns items from the input iterator as long as the test function returns true.


Google knowledge graph search



12-3 sat
PAPER
1. attention mechanism - Bahdanau 2014
neural machine translation -> attention to align and translate
- annotation -> encoder maps input sequence -> intuition: used as a syntactic mapper between english and french
each annotation contains information about whole input sequence 

bi-directional RNN -> transform source sequence in a series of hidden states with equal length -> RNN short term memory since only backpropagate 7 steps -> 

weight alpha is just softmax of exp i.e. associated energy (exp is an alignment model that scores how well inputs and outputs match)
weight -> backpropagable -> probability of which annotations (i.e. is relevant in translating output word) -> weighted sum of all annotations -> expectation over all possible alignments -> alpha reflects importance of annotation with respect to previous hidden state -> attention mechanism in decoder where decoder decides which part of sentence to pay attention to -> relieve the burden of encoder to encode all input sequence into a fixed length vector 


how to determine a model is backpropagable???
why latent variable is not backpropagable???

SOCHER
Socher 2011 unsupervised RNN -> paraphrase detection -> better way to tell document similarity
compare phrases to see if same meaning -> unsupervised rnn and pair-wise sentence comparison of nodes in parsed trees
pairwise comparison of all words in sentence to other sentence
similarity matrix -> captures similairty between nodes and phrases in parsed tree (tree structure is trained by neural network to parse) -> variable sized pooling layer -> why not only root node similarity (work somewhat but not as well as similarity matrix, did lose information as squeeze k-dimentional vector to 1-d vector) -> therefore capture all pair-wise comparison between nodes and phrases -> why not just use similairty matrix to plug into classifier (compared sentences of different length produces different size matrix) -> variable sized pooling layer -> fixed size matrixto plug in neural network -> why min not average pooling -> distance function between compared words i.e. small numbers indicates more similar -> the fact that other disimilar words exist does not hurt the similarity that much therefore min not average -> microsoft research paraphrase corpus

Matrix - vector RNN
compositionality through recursive matrix vector spaces

MV-RNN relationship classification
or other classifier such as RNN, LinMVR, SVM
e.g. cause effect, entity origin, message topic
state of the art: MV-RNN or SVM with tons of features

new compositional model
recursive neural tensor network -> allow more interaction and combination of different words and phrase vectors


given a diagonal line in similarity matrix -> 2 sentence aligh quite well



Writing an essay should be based on reference books. Even human ourselves cannot write an essay from scratch. 
Using math to represent human. But how?


APIAI demo






12-5, 6, 7, 8, 9 
WORKLOG

encoding error when copying from internet -> convert unicode to ascii without errors
see: http://stackoverflow.com/questions/2365411/python-convert-unicode-to-ascii-without-errors
# -*- coding: utf-8 -*-
document = document.decode("utf8").encode('ascii', 'ignore')

sklearn.feature_extraction.text.CountVectorizer
bow is a co-occurence matrix with sentence id as row and word token id as column

DEMO cbow debug

Deadline chatbot presentation file

adjacency matrix is a square matrix used to represent a finite graph -> i.e. similarity matrix  -> speed up look up time of what edges a vertex is connected to
textrank - sentence using networkx
pagerank python implementation

IMPROVING TEXTRANK
PhrazIt lda + lsa + word2vec
LLR x MMR log likeihood ratio + maximal marginal relevance -> see: https://www.youtube.com/watch?v=Vw-7XkP9H1o -> complex question answering based on multiple documents + query
simplfying sentences 
annotated suffix tree


12-12, 13, 14 15 16
WORKLOG
LSA - SVD intuition + implementation


12-19,20,21,22,23
next week tasks: 
1. LSA speed up
2. noun phrases machine learning extraction
3. taxonomy/relation extraction -> categorization
4. text rank weighted graph

others: 
categorization -> NER, categorization 
word2vec wiki cbow -> set + uniquely belong to a word
Glove (maybe)















EXAMS (linear algebra + CMU15-213)
RULE: hours based: 8-10:A, 5-7:B

12-16 3pts
15-213 lecture7  
passing control - stack pointer, arguments passed to registers, program counter, call pop procedure, rax + rbx compute, rdi rsi rdx rcx r8 r9 store arguments, stack to store state of function, registers store value across memories for sharing
passing data + managing local data - rbp frame pointer 
recursion -> multiple simultaneous instantiations of single procedure i.e. reentrant -> state of instantiation -> arguments, local variable, return pointer -> one function executing at any given time, other function frozen -> 
stack frame = state for single procedure instantiation
stack store state for one function in recursion -> when function finished execution, all states popped, make room for new function
$rbp -> frame pointer/base pointer (optional pointer)


bomb lab 
see: http://web.eecs.umich.edu/~sugih/pointers/gdbQS.html

gdb, objdump + understand assembly + procedure + control flow + memory during execution
strings bomb > bomb-strings, objdump -d > bomb-assembly
break main.cpp:55, break main, break *0x400e87, delete 1, 0000000000400e87 = 0x400e87, 
run, r, step, s, next, n, continue, c, stepi take 1 instruction step
break phase_1 -> stepi + disas + info registers (i r)
until *0x

p/x $eax -> print
x /25c 0x400e87 -> examine
x /NUM SIZE FORMAT
x /24wx $rsp
x /s $rdi -> loook at 1st argument as string
x /d $rdi -> as integer

0x400ed4 <+72> je/jne 0x400eba<phase_2+46) (smaller address means loop + 46 is where the loop starts)

esi vs rsi -> esi set upper 32bits to 0

leaq -> create pointer i.e. instead of putting the value in memory into rdi, create a memory reference i.e. pointer in rdi -> movq (memory reference of 8(%rsp)) %rdi

%rax seems to the value sharing by functions via return /return value
ret instruction always takes what stack pointer pointed to as return address

callee-saved i.e. callee must save & restore
when trying to alter, callee save the value on stack

pushq %rbx -> callee saved push saved $rbx value on stack 

cmpg/jne/testq -> conditional statement 
testq-> only purpose is to set condition flag/code in register

ABI = application binary interface  

list
disas -> no frame selected ???? why
info registers -> rsp stack pointer, rip instruction pointer, rax holds return, rsi rdi holds arguments of function

12-17 sat -5pt
bomb lab 4pts
lecture7 1pt

12-18 sun -6pts  
stack discipline + recursion -> each recursed function will have its own stack frame rather than sharing the same stack

average run of the mill programmer - i am not even close to that
single byte register
compg %rsi, %rdi
setg %al # set 1 byte of %rax when greater than
movzbl %al, %eax # zero out all except lowest order bit of %rax

%eax mean %rax with upper 32 bytes set to 0

%rdi = 1st argu
%rsi = 2nd argu
calling function will go to %rax to look for return value

12-19 -2pts
matrix -> eigenvalue -> determinant -> reduced row echelon form to get to identity matrix -> linear transformation: transformation matrix S i.e. linear transformation on identity matrix -> manuel vs formal math matrix form -> determinant related to inverse -> sub-matrix
linear transformation = manuel = row operation
A * A.I = I
determinant = ad-bc -> whether matrix is invertible -> Det(A) = |A| 

linearly dependent column = not invertible = det =0 = linear combination of each other


12-20 -2.5pts
det(lamda * I - A) = 0

12-21 -5pts
nullspace
orthogonal
diagonalization
rank
U*Z -> unit vector * sigma the stretching vector

why SVD -> gives best axis to project on -> min sum of squares of projection errors -> minimum reconstruction error
best low rank k approximation -> min||A-B|| where rank(B) = k 

sigma -> strength of the import, variance spread on first right singular vector axis, importance of that vector

V -> axis to project the points
UZ -> coordinates of the points in projection axis

PCA 
see: https://www.youtube.com/watch?v=F-nfsSq42ow
trying to relate SVD to PCA with dimentionality reduction
many ways to do dimentionality reduction -> most popular way is PCA and SVD

statistically dependent -> redundancy

spectral decomposition 
matrix factorization



12-22 3.5 pts
attack-lab read up

12-23 2.5pts
hierarchical softmax

negative sampling
see: http://stats.stackexchange.com/questions/176658/negative-sampling-expectation-in-word2vec-algorithm

12-24 2pts WTF entire day wasted on fitness and pancake and how i met your mother
WORKLOG
andrew ng - dev set test set

attack lab 
gadget sequences of bytes that ends in c3 (ret)
pop + move + constant 

12-25 5pts WTF AGAIN 
different version of gcc and different compiler settings
leaq directly computes the the value referred by memory address rather than doing memory address arithmetic i.e. &x[i]
salq -> shift left e.g. salq $2, %rax -> shift left by 2 = multiply by 4 in binary

driving lesson
fiction zone, clutch, throttle/gas, brake
throttle 1.5-2 rpm, not rafting up to 3 rpm and dump it
1.5 throttle hold + clutch release -> start the car

1st gear + clutch with brake to stop

be smooth on/off throttle in 1st gear as ratio so small

1st gear -> 2nd gear
3.5 rpm -> lift off throttle + clutch + switch to 2nd gear -> keep at 2-3 rpm

speed bump 
brake til 1.5 rpm -> clutch + shift to 1st gear -> clutch + throttle til 3.5 + shift to 2nd gear

2nd -> 3rd 
4 rpm -> clutch in off the gas -> shift to 3rd gear -> throttle + release clutch

3rd -> 2nd
clutch in -> 3rd to 2nd + blip -> release clutch -> brake 

i.e. up a gear -> lift
i.e. down a gear -> blip

normal driving -> 1.5 - 2 rpm below 3.5


reverse 

hill start

rpm = revolution per minute = rev/min
matching rev
torque -> moment of force -> 1st gear lots of torque because of short gearing but cant reach a high speed -> given same speed, higher gear lower rpm i.e. more torque/power to main same rpm -> at given speed, different gear = different rpm -> clutch disk spine the transmission connect to the gear -> pressure plate connected to flywheel, always rotating at engine's speed -> e.g. 3rd to 2nd, given same speed, downshift gear cause clutch disk spin higher rpm, therefore blip engine to rev match speed of clutch disk

when to shift gear
higher GR = Higher Torque of wheel
2nd gear to turn
1st gear to up hill / start / stop / slow traffic crawling

why and when stall ? i.e. engine fails to turn drive shaft
-> in general, fighting its own pumping losses, resistances of car wheels, entire drive shaft it tries to turn
-> Engines have very little power and torque at idle speed. When the wheels stop, the engine has to stop as well, unless the engine is decoupled by using the clutch -> i.e. the engine stops running when it runs below 0.5rpm -> either clutch disengage /+ neutral gear
-> low speed, high gear -> first shake, then stall
-> when stop must be 1st gear otherwise stall ????? because you release gear ???? -> start after stop need to switch to 1st gear
-> release too quick stall -> engine were not able to engage clutch disk to rotate, bite point prevent stalling
-> release clutch without putting gas/moving stall


feet off clutch, in gear, engine brake, engine slowing the car down by being in gear
coasting -> clutch pressed driving -> free-wheel and not use the engine to move
leaving the clutch down whilst making left or right turns is called coasting

block gear changing -> no need to change down 3 gears when stoping
when stop-> change to 1st gear otherwise car will stall -> i.e. wheel stopped but engine still moving

when on hill build up speed and momentum before up gear
-> e.g. when turning right/up hill + low speed -> if not 1st gear -> car will shake due to pressure plate cannot move clutch disk 

hand brake + neutral

12-26 -2pts - driving day
stack discipline -> callee saved -> always pushq %rbx + subq $16, %rsp -> addq $16, %rsp + popq% rbx -> saved %rbx value from call function, restore/pop the saved value on stack back to %rbx -> every function will treat %rbx this way unless it is not going to alter


Linear Algebra
PCA vs SVD 
see: http://www3.cs.stonybrook.edu/~sael/teaching/cse549/Slides/CSE549_16.pdf
see: http://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca
PCA finds a linear projection of high dimensional data into a lower dimensional subspace
 
12-27 - 5pts
space, determinant, inverse


12-28 -club
12-29 -club

12-30 - 1pt
learn c - ex16
pointers to struct + malloc
strdup vs strcpy see: http://stackoverflow.com/questions/14020380/strcpy-vs-strdup


12-31 -4pts
linear dependence = not invertible = det(A) = 0


1-1 - 6pts
rank, eigenvalue, eigenvector, matrix factorization

1-2

1-7 - 5pts

1-10 -2pts
forget gate -> either 1 or 0 thats why any change of x would affect utimate value, if 0 does not affect, if 1 then no vanish/explode

1-11
1-12
1-13
evlaution + dev set in word2vec
pickle vs joblib vs hfd5 vs csv
multiple words searches in lsa

1-14 sat
chatbot

1-15 sun
chatbot


1-16 mon
evaluation

1-17 tues


Linear Algebra
looking for patterns in matrix
subspace = span = linear extension through negative plane

linear combination + span 
see: https://www.youtube.com/watch?v=Qm_OS-8COwU
=> vector times scalar -> linear as just scaling them up not multiply vectors with each other -> span = sets of all linear combination of vectors -> any vector in R2 could be combination of 2 vectors -> negative to cover the all 4 quaters
-> i.e. span(V1, V2) = R2
"Any n-number of linearly independant (not colinear) vectors in R^n space span that whole (R^n) space"

see: https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/linear-independence/v/more-on-linear-independence
linear independent -> no way to represent one by another / no linear combination of all vectors where at least 1 constant != 0 could result in a 0 vector
linear independent set -> combination of sets of vectors -> algebra to solve and 0 is the only solution

"span is looking for linear independence"
"could we describe the whole space (R^n) with n-number (not n-dimentional) vectors" -> i.e. given only 2 equations and answers, we do not have a solution for 3 unknowns -> span graphical represents this
"given a set of anwsers and solution, there is always a combination of constants/parameters to solve"  


Linear Subspace
see: https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/subspace-basis/v/linear-subspaces
1. contains 0 vector
2. closure under scalar multiplication
3. closure under addition
span(V1, V2, V3) = valid subspace of R^3
span([1,1]) = valid subspace of R^3 
subspace = span of linear dependent/colinear vectors
basis of subspace = no redundency



Learn C - ex17 heap stack allocation
C program is like an interface/api/abstraction layer to memory
fopen
fread
fwrite
malloc
free
fflush
strncopy
atoi

stream - a pointer to a FILE object that identifies the hard disk memory (stream)
database -> a buffer to store string from fread input FILE object


Datalab - Bitwise operator on integers (not float) -> bitwise arithmetic = binary arithmetic

complement operation -> ~2 = -3 -> simply flip the bits of 2 = two's complement representation of -3
see: http://stackoverflow.com/questions/791328/how-does-the-bitwise-complement-operator-work
negative number is stored as 2's complement(inverting all the bits) of positive counterpart -> add 1 ->first bit is a sign bit

1. bitXor - x^y using only ~ and & 

2.


ONGOING
::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

CHATBOT
IT IS A PHILOSOPHICAL QUESTION 
YOU COULD KEEP TWEAKING THE MATH BASED ON CHANGING PREMISES AND ASSUMPTIONS
BUT THE REAL PART IS HOW TO BEAT GOOGLE (SO YOU COULD MONETIZE)
IT IS A PHILOSOPHICAL QUESTION WHAT THESE MODEL HAS TO DO WITH DATA AND HUMAN ITSELF
IT IS NO LONGER JUST A MATH QUESTION
GOOGLE IS TRUSTED BECAUSE OF ORGANIZATION NAME -> TOP 10 MOST POPULAR ORGANIZATIONAL RESULTS E.G. CNN WWN
THE NATURE OF THE WORK ITSELF IS COMPETING WITH GOOGLE
THE NATURE OF THE WORK ITSELF IS CRAZY
SO ASK YOURSELF 
HOW CRAZY ARE YOU WILLING TO DREAM? 
GOAL-> WE MODEL OUR FRIENDS -> USE THEIR DIARY HOMEWORK WRITING 
BULIDING A CHATBOT IS LIKE BUILDING FACEBOOK PROFILE




CHATBOT
automatically divide the data up to different context/emotions/intents based on presence of certain words


DEADLINE:
Bluesky
Hackhorizons
WorkApplications
yahoo japan deadline12-15
9gag
missed accenture
azeus systems

::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
ONGOING



Language model (computes the probability for a sequence of words)
machine translation -> word ordering + word choices
RNN -> label could be next word or sentiment 
vanishing and exploding gradient
NER
entity level sentiment in context
opinionated expressions


MACHINE LEARNING LEARNING GOALS:
neural network -> regularization to solve overfitting of data









Mid-term:
SVM
softmore
neural
logistic
linear





+41pts




Paper topics:
1. reduce system crashes due to drivers + standardize interfaces to these devices
2. a new matching algorithm to match applicants to college in hong kong
3. download if stop or interrupted, no loss of data
4. new compiler and language to write operating system since multithreading cannot be implemented with just library + new ways for website to sequentially load up images and video e.g. youtube, a different way to do advertisement for multithreading 
 
Stupid topics:
1. Multi-threading in C using library that is later added does not work - but how to make it work though?












DATABASE PRINCIPLES
1. generate id by push
2. denormalization or nesting
3. uidmapping with username
4. atomic write, multiple updates
5. duplicate 


Firebase Android Sample Instagram 
https://github.com/firebase/friendlypix/tree/master/android/app/src/main/java/com/google/firebase/samples/apps/friendlypix







THINK ABOUT:
?? Toolbar setSupportActionBar(toolBar)
MenuItem
ViewPager FeedsPagerAdapter Tablayout
mFab = (FloatingActionButton)
login token
Glide.with(MainActivity.this).load(friendlyMessage.getPhotoUrl()).into(viewHolder.messengerImageView)
LinearLayout linLayout = (LinearLayout)findViewWithTag("layout1");
dynamic view
View rootView = inflater.inflate(R.layout.fragment_main, container, false);
AlertDialog.Builder builder = new AlertDialog.Builder(getActivity(), R.style.CustomTheme_Dialog);
LayoutInflater inflater = getActivity().getLayoutInflater();
public Dialog onCreateDialog(Bundle savedInstanceState) 
        Bundle bundle = new Bundle();
        addListDialogFragment.setArguments(bundle);
public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
    }
getDialog().getWindow().setSoftInputMode(WindowManager.LayoutParams.SOFT_INPUT_STATE_VISIBLE);
public void onActivityCreated(Bundle savedInstanceState) 
Inheritance: extends AppCombatActivity implements GoogleApiClient.OnConnectionFailedListener{ALL CODE IN THIS PUBLIC ABSTRACT CLASS};
intent.setFlags(Intent.FLAG_ACTIVITY_NEW_TASK | Intent.FLAG_ACTIVITY_CLEAR_TASK);
final SharedPreferences sp = PreferenceManager.getDefaultSharedPreferences(BaseActivity.this);
menu item xml+ onCreateOptionsMenu(Menu menu) onOptionsItemSelected(MenuItem item) -> show as action
Fragment vs Dialogue fragment
public static ShoppingListsFragment newInstance(String encodedEmail) -> use this factory method to create an instance of this fragment
widget -> floating action button
public static ShoppingListsFragment newInstance // Create fragment and pass bundle with data as fragment's arguments
mListView.setOnItemClickListener(new AdapterView.OnItemClickListener() -> what is adapterView
ShoppingList selectedList = mActiveListAdapter.getItem(position); // ?? there is no getItem() or getRef() + why does it return a model ShoppingList -> shoppinglistfragment.java
textViewCreatedByUser.setText(mActivity.getResources().getString(R.string.text_you));
mActivity.getResources() vs Firebase ref
        <android.support.v7.widget.Toolbar
HashMap<String, Object> itemToAdd =
          (HashMap<String, Object>) new ObjectMapper().convertValue(itemToAddObject, Map.class);
public static AddListItemDialogFragment newInstance that creates new bundle vs protected static Bundle newInstanceHelper that pass in bundle
protected void doListEdit() of AddListItemDialogFragment -> where is it called ?
Firebase path update
// deep-path update
            HashMap<String, Object> shoppingListMap = (HashMap<String, Object>)
                    new ObjectMapper().convertValue(newShoppingList, Map.class);
Utils.updateMapForAllWithValue(null, listId, mEncodedEmail,
                    updateShoppingListData, "", shoppingListMap);




NATIVE VS IOS ANDRIOD

database set up
web api to send news/ads to front end 
nodejs vs python to set up server -20+pts
authentication -20+pts
db mongodb? any other choices? -10pts
android pushing the limit pdf
android big ranch pdf




Silicon Valley: system architecture, networking, security, getting root access to NSA servers, database rollback

Reading design decisions -> 2 pts

pointer arithmetic, type casting, malloc, realloc, free, sbrk, brk, mmap, memmove, memset-> 12 pts
open(), strdup(), typedef. 
int putchar(int c) // putc(c, stdout)
int fputc(int c, FILE* stream); // writes the character, cast unsigned char to stream
int putc(intc, FILE* stream);
euler number is irrational and transcendental




